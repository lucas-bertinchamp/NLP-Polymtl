{"cells":[{"cell_type":"markdown","metadata":{"id":"UG1IIiGzZSXy"},"source":["## <center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n","## <center> TP3 - Interprétation de requêtes SPARQL par des questions en langue naturelle <br>  Automne 2023"]},{"cell_type":"markdown","metadata":{"id":"Z2UzJAiw-8su"},"source":["## Identification de l'équipe:\n","\n","### Groupe de laboratoire: 02\n","\n","### Equipe numéro : 13\n","\n","### Membres:\n","\n","- Sebastian Villanueva (2087346) (33% de contribution, préparation des données, segmentation, batching)\n","- Lucas Bertinchamp (2312324) (33% de contribution, transformer entrainement)\n","- Antoine Toussaint (2312379) (33% de contribution, évaluation)\n","\n","* nature de la contribution: Décrivez brièvement ce qui a été fait par chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail."]},{"cell_type":"markdown","metadata":{"id":"uJTHc5UnZSX0"},"source":["## Description\n","\n","Dans ce laboratoire, vous allez construire un traducteur automatique en utilisant l'architecture du Transformeur. L'idée est d'utiliser un système de traduction automatique pour traduire des requêtes en langage SPARQL vers des questions en anglais.\n","\n","SPARQL est un langage d'interrogation de bases de connaissances, similaire à SQL. Les bases de connaissances sont une source de données structurées, selon les standards, modèles et langages du Web sémantique, qui permettent un accès efficace à une grande quantité d'information dans des domaines très variés. Cependant, leur accès est limité par la complexité des requêtes qui ne permet pas au public de s'en servir directement. Il est aussi difficile pour l'usager non averti de comprendre le sens d'une requête. Nous voulons donc coder un modèle de type Transformer qui permette d'interpréter une requête SPARQL sur la base de connaissances DBpedia en lui associant une question en anglais.\n","\n","Ainsi, notre système de traduction automatique prendra en entrée une requête SPARQL et produira en sortie une phrase en anglais correspondant à la question qui est posée par la requête. Par exemple :\n","\n","__Entrée__ _select distinct count ( ?uri ) where { dbr:Apocalypto dbo:language ?x . ?x dbp:region ?uri }_\n","\n","__Sortie attendue__ : _In how many other dbp:region do people live, whose dbo:language are spoken in dbr:Apocalypto?_\n","\n","Vous avez pu constater qu'on réutilise des éléments avec le préfixe dbr /dbo/dbp qui sont associés aux données dans DBpedia et au schéma de la base de connaissances. dbr:Apocalypto est tout simplement une URI qui décrit une ressource (ou donnée) dans DBpedia. Voici l'URI en question: https://dbpedia.org/describe/?url=http%3A%2F%2Fdbpedia.org%2Fresource%2FApocalypto&sid=35407\n","\n","Dans ce TP, vous reproduirez l'architecture du Transformer à l'aide de couches Keras. Vous pouvez vous inspirer de l'implémentation de certaines méthodes du tutoriel [Tensorflow](https://www.tensorflow.org/text/tutorials/transformer)"]},{"cell_type":"markdown","metadata":{"id":"ga8nmtY4ZSX1"},"source":["## LIBRAIRIES PERMISES\n","- Jupyter notebook\n","- NLTK\n","- Numpy\n","- Pandas\n","- Sklearn\n","- Tensorflow\n","- Keras\n","- Transformers\n","- Datasets\n","- Pour toute autre librairie, demandez à votre chargé de laboratoire"]},{"cell_type":"markdown","metadata":{"id":"Qi1tFmIZZSX1"},"source":["## INFRASTRUCTURE\n","\n","- Vous avez accès aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)\n","- Vous pouvez aussi utiliser Google Colab."]},{"cell_type":"markdown","metadata":{"id":"rHCvEjauZSX2"},"source":["## DESCRIPTION DES DONNÉES ET MÉTRIQUES D’EVALUATION\n","\n","Le corpus est un corpus de 5 000 paires de questions - requêtes sur DBPedia portant sur une grande variété de thèmes plus ou moins spécifiques. Trois ensembles de données sont fournis :\n","\n","- Les 4000 paires de questions – requêtes d’entrainement dans un fichier `train.csv`.\n","- Les 500 paires de questions – requêtes de validation dans un fichier `validation.csv`.\n","- Les 500 paires de questions - requêtes de test dans un fichier `test.csv`\n","\n","La métrique BLEU sera utilisée pour comparer les traductions des modèles aux requêtes de référence."]},{"cell_type":"markdown","metadata":{"id":"uIZfzAD_ZSX2"},"source":["## LABORATOIRE"]},{"cell_type":"code","execution_count":125,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1092,"status":"ok","timestamp":1698289655976,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"N9-4PatU_jxi","outputId":"2e2d024a-dd61-42b5-e029-fecc129cb205"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4767,"status":"ok","timestamp":1698289660742,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"AHulKdPZZSX2","outputId":"33968792-fcb4-4cc7-88e6-b6a2436607f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.15.0)\n","Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.2.2)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","!pip install tensorflow_text\n","import tensorflow_text\n","import pathlib\n","import re\n","from nltk.translate.bleu_score import sentence_bleu\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","execution_count":127,"metadata":{"id":"-46mweTdajyw","executionInfo":{"status":"ok","timestamp":1698289660742,"user_tz":240,"elapsed":14,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["root = ''"]},{"cell_type":"markdown","metadata":{"id":"GvMsQC9IZSX3"},"source":["### 1 Préparation des données (10 points)\n","\n","Il faut tout d'abord préparer les données avant de les envoyer au système de traduction. Pour cela, deux classes seront utilisées. La classe `DataLoader` servira simplement à lire les données des fichiers d'entrainement et de validation et la classe `Preprocessor` servira à pré-traiter les données dans un format attendu"]},{"cell_type":"code","execution_count":128,"metadata":{"id":"Yk674QnrZSX4","executionInfo":{"status":"ok","timestamp":1698289660742,"user_tz":240,"elapsed":13,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class DataLoader:\n","    \"\"\"\n","    Classe servant à charger les données en DataFrame\n","    \"\"\"\n","\n","    def __init__(self, training_path: str, validation_path: str) -> None:\n","        # Initialise les attributs .train et .val en chargeant\n","        # les données à partir des chemins d'accès donnés en paramètre\n","        self.train = pd.read_csv(training_path)\n","        self.val = pd.read_csv(validation_path)"]},{"cell_type":"markdown","metadata":{"id":"BSm4a5St-8sy"},"source":["#### 1.1 Pré-traitement\n","\n","La classe `Preprocessor` effectuera les transformations suivantes sur les requêtes SPARQL :\n","- Remplacer tous les mots clés (préfixes) de la forme `dbx:` par `dbx_` (par exemple, `dbr:` devient `dbr_` et `dbo:` devient `dbo_`). Les mots clés qui doivent être pris en compte sont les suivants : `dbr`, `dbo`, `dbp` et `rdf`\n","- Remplacer tous les signes de ponctuation suivants par des mots :\n","  - `?` deviendra `var_`\n","  - `{` deviendra `brack_open`\n","  - `}` deviendra `brack_close`\n","  - `(` deviendra `parent_open`\n","  - `)` deviendra `parent_close`\n","  - `.` deviendra `sep_dot`\n","\n","En ce qui concerne les questions en anglais, La classe `Preprocessor` effectuera les transformations suivantes :\n","- Enlever les `?` à la fin des phrases\n","- Remplacer tous les mots clés de la forme `dbx:` par `dbx_` (par exemple, `dbr:` devient `dbr_` et `dbo:` devient `dbo_`). Les mots clés qui doivent être pris en compte sont les suivants : `dbr`, `dbo`, `dbp` et `rdf`\n","- Enlèvera tous les espaces inutiles avant le début et après la fin de la question\n","\n","Cette classe s'occupe aussi d'annuler le pré-traitement une fois que le Transformer aura généré une séquence, ce qui inclut notamment d'annuler les transformations indiquées ci-dessus et d'enlever les jetons de début et de fin de phrases qui auront été ajoutés par le segmenteur un peu plus bas."]},{"cell_type":"code","execution_count":129,"metadata":{"id":"20IAnMV4ZSX4","executionInfo":{"status":"ok","timestamp":1698289660742,"user_tz":240,"elapsed":12,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Preprocessor:\n","    \"\"\"\n","    Transforme et nettoie les données pour améliorer les performances du modèle\n","    \"\"\"\n","\n","    SPARQL_TRANSLATE_OBJECTS = {\n","        \"dbr:\": \"dbr_\",\n","        \"dbo:\": \"dbo_\",\n","        \"dbp:\": \"dbp_\",\n","        \"rdf:\": \"rdf_\"\n","    }\n","\n","    SPARQL_TRANSLATE_SYMBOLS = {\n","        \"?\": \"var_\",\n","        \"{\": \"brack_open\",\n","        \"}\": \"brack_close\",\n","        \"(\": \"parent_open\",\n","        \")\": \"parent_close\",\n","        \".\": \"sep_dot\"\n","    }\n","\n","    def transform_dataframe(self, data: pd.DataFrame):\n","        \"\"\"\n","        Transforme les données d'une DataFrame contenant les colonnes 'english'\n","        et 'sparql'. Fait appel aux fonctions `transform_sparql` et\n","        `transform_english` sur les bonnes colonnes\n","\n","        Args :\n","            - data : Données à transformer\n","\n","        Returns :\n","            Données transformées\n","        \"\"\"\n","        data['english'] = data['english'].apply(self.transform_english)\n","        data['sparql'] = data['sparql'].apply(self.transform_sparql)\n","        return data\n","\n","    def transform_sparql(self, sparql: str):\n","        \"\"\"\n","        Transforme une requête sparql en remplacant les jetons \"dbx:\" par \"dbx_\"\n","        et en remplacant les signes de ponctuation par leur équivalent en mots\n","        tel qu'indiqué plus haut\n","\n","        Args :\n","            sparql : Requête sparql\n","\n","        Returns :\n","            Requête sparql transformée avec les modifications mentionnées plus haut\n","        \"\"\"\n","        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n","            sparql = sparql.replace(key, value)\n","        for key, value in self.SPARQL_TRANSLATE_SYMBOLS.items():\n","            sparql = sparql.replace(key, value)\n","        return sparql\n","\n","    def transform_english(self, english: str):\n","        \"\"\"\n","        Transforme une requête sparql en remplacant les jetons \"dbx:\"\n","        par \"dbx_\" et en enlevant les points d'interrogation ainsi que\n","        les espaces non-nécessaires au début et à la fin de la phrase\n","\n","        Args :\n","            - english : Phrase en anglais sur laquelle appliquer\n","            les transformations\n","\n","        Returns :\n","            Phrase transformée avec les modifications mentionnées plus haut\n","\n","        \"\"\"\n","        for key, value in self.SPARQL_TRANSLATE_OBJECTS.items():\n","            english = english.replace(key, value)\n","        english = english.replace(\"?\", \"\")\n","        english = english.strip()\n","        return english\n","\n","    def transform_back_english(self, english):\n","        \"\"\"\n","        Effectue les transformations inverses de la phrase en anglais\n","        (remplace les dbx_ en dbx:).\n","        Attention, cette fonction doit aussi enlever les jetons de début\n","        et de fin d'une phrase qui sont ajoutés lors de\n","        la segmentation (tokenization)\n","\n","        Args :\n","            - english : Phrase générée par un modèle contenant les jetons\n","            de début et de fin\n","\n","        Returns :\n","            - Phrase en anglais dont les transformations ont été annulées\n","        \"\"\"\n","        # Cette fonction est appelée après la fonction detokenize de la partie 2, qui elle-même appelle la fonction cleanup_text\n","        # La fonction cleanup_text est déjà appelée pour enlever les jetons de début et de fin avant d'appeler cette fonction !\n","        # Il faut donc seulement remplacer les dbx_ en dbx:\n","        # Attention, quand detokenize de BertTokenizeur est utilisé, les jetons sont séparés par des espaces ! ('dbr _ ' et non 'dbr_')\n","\n","        english = english.decode(\"utf-8\", \"ignore\")\n","        english = english.replace(\"dbr _ \", \"dbr:\")\n","        english = english.replace(\"dbo _ \", \"dbo:\")\n","        english = english.replace(\"dbp _ \", \"dbp:\")\n","        english = english.replace(\"rdf _ \", \"rdf:\")\n","\n","        # Il faut aussi enlever les espaces avant et après le symbole _\n","        english = english.replace(\" _ \", \"_\")\n","\n","        return english\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NhO7WtxL-8sz"},"source":["Vous pouvez vérifier votre implémentation de la classe `Preprocessor` à l'aide du test suivant"]},{"cell_type":"code","execution_count":130,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1698289660742,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"dgiPpZZt-8sz","outputId":"492fdb13-d650-4f8e-b6a6-a1a0997b2e1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed sparql : \n","select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close\n","select distinct var_uri where brack_open var_uri dbo_founder dbr_John_Forbes_parent_openBritish_Army_officerparent_close sep_dot var_uri rdf_type dbo_City brack_close\n","\n","Transformed english : \n","how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n","what dbo_City's dbo_founder is dbr_John_Forbes_(British_Army_officer)\n"]}],"source":["def test_preprocessor():\n","\n","    test_queries = [\n","        'select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . }',\n","        'select distinct ?uri where { ?uri dbo:founder dbr:John_Forbes_(British_Army_officer) . ?uri rdf:type dbo:City }'\n","    ]\n","\n","    test_english = [\n","        'how many movies are there whose dbo:director is dbr:Stanley_Kubrick ?',\n","        'what dbo:City\\'s dbo:founder is dbr:John_Forbes_(British_Army_officer) ?'\n","    ]\n","\n","    preprocessor = Preprocessor()\n","    print('Transformed sparql : ')\n","    for query in test_queries:\n","        print(preprocessor.transform_sparql(query))\n","\n","    print()\n","    print('Transformed english : ')\n","    for english in test_english:\n","        print(preprocessor.transform_english(english))\n","\n","\n","test_preprocessor()"]},{"cell_type":"markdown","metadata":{"id":"NElXb1XP-8sz"},"source":["Sortie attendue :\n","\n","```\n","Transformed sparql :\n","select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close\n","\n","select distinct var_uri where brack_open var_uri dbo_founder dbr_John_Forbes_parent_openBritish_Army_officerparent_close sep_dot var_uri rdf_type dbo_City brack_close\n","\n","Transformed english :\n","how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n","\n","what dbo_City's dbo_founder is dbr_John_Forbes_(British_Army_officer)\n","```"]},{"cell_type":"markdown","metadata":{"id":"uyvTqTeu-8s0"},"source":["Vous pouvez maintenant instancier une objet de la classe `Data Loader` pour charger les données d'entrainement et de validation à partir des fichiers `train.csv` et `validation.csv`"]},{"cell_type":"code","execution_count":131,"metadata":{"id":"3L0LoK9-ZSX4","executionInfo":{"status":"ok","timestamp":1698289660742,"user_tz":240,"elapsed":8,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["# Instancier une objet de la classe DataLoader pour charger les données\n","data_loader = DataLoader(\n","    training_path=root + 'train.csv',\n","    validation_path=root +'validation.csv'\n",")"]},{"cell_type":"markdown","metadata":{"id":"0UBkqj2J-8s0"},"source":["Appliquez le pré-traitement des données sur les données chargées précédemment"]},{"cell_type":"code","execution_count":132,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698289660742,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"d1eNYoOTZSX4","outputId":"bcb62afc-107f-4171-85a8-226aa64a5287"},"outputs":[{"output_type":"stream","name":"stdout","text":["how many movies are there whose dbo_director is dbr_Stanley_Kubrick\n"]}],"source":["# Appliquer le pre-processeur sur les données d'entrainement et de validation\n","pre_processor = Preprocessor()\n","processed_train = pre_processor.transform_dataframe(data_loader.train)\n","processed_val = pre_processor.transform_dataframe(data_loader.val)\n","print(processed_train.get('english').iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"ab9aHTYkZSX5"},"source":["### 2. Segmentation (tokenization) (15 points)\n","\n","Une fois les données importées et modifiées, il faut adapter les phrases dans un format que le modèle peut comprendre.\n","\n","Tout d'abord, il faudra segmenter les phrases en jetons. Pour cela, un dictionnaire de mots (vocabulaire) sera nécessaire."]},{"cell_type":"markdown","metadata":{"id":"LWiWaryF-8s0"},"source":["#### 2.0 LanguageTokenizer (10 points)\n","\n","La classe `LanguageTokenizer` s'occupera de créer ce vocabulaire et de transformer les phrases d'un langage spécifique en jetons. Dans notre cas, il y aura 2 instances de cette classe : une pour l'anglais et l'autre pour sparql. Cette classe possède plusieurs fonctions qui nous seront très utiles notamment `create_vocab` pour créer le vocabulaire du modèle, `tokenize` pour transformer les phrases en jetons et `detokenize` pour transformer les jetons en phrases.\n","\n","Nous allons avoir recours au segmenteur de Bert pour trouver les jetons et le vocabulaire. Les paramètres du segmenteur vous sont donnés. Ce segmenteur divise chaque mot en parties de mots. Par exemple \"characteristically\" sera segmenté en 'characteristic' et '##ally'.\n","\n","Ensuite, pour chacune des phrases, après les avoir transformées en jetons, il faudra ajouter les jetons de début (`[START]`) et de fin de phrase (`[END]`). Cette opération sera effectuée dans la fonction `add_start_end`."]},{"cell_type":"code","execution_count":133,"metadata":{"id":"aQsquspb7esn","executionInfo":{"status":"ok","timestamp":1698289660742,"user_tz":240,"elapsed":7,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class LanguageTokenizer(tf.Module):\n","    \"\"\"\n","    Classe représentant un tokenizer pour un langage spécifique.\n","    Dans notre cas, il y en aura un pour sparql et un pour l'anglais\n","    \"\"\"\n","\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","    START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","    END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","    tokenizer_params = dict(lower_case=True)\n","\n","    vocab_args = dict(\n","        vocab_size = 8000,\n","        reserved_tokens=reserved_tokens,\n","        bert_tokenizer_params=tokenizer_params,\n","        learn_params={},\n","    )\n","\n","    def __init__(self, reserved_tokens, vocab_path):\n","        \"\"\"\n","        Initialise le BertTokenizer en utilisant le paramètre `vocab_path`\n","        et en mettant le tokenizer en mode \"lower case\".\n","\n","        Args :\n","            - reserved_tokens : Jetons réservés du BertTokenizer\n","            - vocab_path : Chemin vers le fichier contenant le vocabulaire du tokenizer\n","        \"\"\"\n","        self.vocab_path = vocab_path\n","        self.reserved_tokens = reserved_tokens\n","        self.bert_tokenizer = tensorflow_text.BertTokenizer(vocab_path, lower_case=True)\n","\n","    def create_vocab(language_sentences: pd.DataFrame, path: str):\n","        \"\"\"\n","        Crée un vocabulaire à partir des phrases en entrée\n","        (language_sentences). Pour cela vous devrez utiliser\n","        la fonction bert_vocab_from_dataset(). Attention, il\n","        ne faut pas oublier de passer en paramètres `vocab_args`\n","        à la fonction qui s'occupe de créer le vocabulaire.\n","\n","        Une fois le vocabulaire créé, il faudra le sauvegarder dans un fichier spécifié\n","        par l'attribut `path`.\n","\n","        Args :\n","            - language_sentences : DataFrame contenant les phrases du langage\n","            - path : Chemin où sera sauvegardé le vocabulaire\n","        \"\"\"\n","        language_sentences = tf.data.Dataset.from_tensor_slices(language_sentences)\n","        vocabulary = bert_vocab.bert_vocab_from_dataset(\n","            language_sentences,\n","            **LanguageTokenizer.vocab_args\n","        )\n","\n","        pathlib.Path(path).write_text(\"\\n\".join(vocabulary))\n","\n","    @tf.function\n","    def tokenize(self, inputs):\n","        \"\"\"\n","        Transforme des phrases en index de jetons et qui ajoute les\n","        jetons de début et de fin.\n","\n","        Args :\n","            - inputs : Phrases d'entrée\n","\n","        Returns :\n","            Jetons correspondant à la phrase avec les jetons de début et de fin\n","        \"\"\"\n","        tokens = self.bert_tokenizer.tokenize(inputs).merge_dims(-2,-1)\n","        tokens = LanguageTokenizer.add_start_end(tokens)\n","        return tokens\n","\n","    @tf.function\n","    def detokenize(self, tokenized):\n","        \"\"\"\n","        Transforme une liste d'index en jetons. Applique ensuite\n","        la méthode `cleanup_text` du pour nettoyer\n","        les données.\n","\n","        Args :\n","            - tokenized : Liste de jetons\n","\n","        Returns :\n","            Phrase correspondant aux jetons\n","        \"\"\"\n","        tokens = self.bert_tokenizer.detokenize(tokenized)\n","        cleaned = LanguageTokenizer.cleanup_text(self.reserved_tokens, tokens)\n","        return cleaned\n","\n","    #@tf.py_function(Tout=tf.RaggedTensorSpec(dtype=tf.int64, ragged_rank=1))\n","\n","    def add_start_end(tokenized_sentences):\n","        \"\"\"\n","        Fonction qui ajoute la représentation des tokens [START] et [END] à la phrase en entrée\n","\n","        Args :\n","            - tokenized_sentences: Tenseur contenant les indices des jetons des phrases\n","\n","        Returns :\n","            Tenseur initial avec les indices des jetons [START] et [END] au début et à la fin\n","        \"\"\"\n","        #length = len(tokenized_sentences.to_tensor())\n","        length = tokenized_sentences.nrows()\n","        start = tf.fill([length, 1], LanguageTokenizer.START)\n","        end = tf.fill([length, 1], LanguageTokenizer.END)\n","        return tf.concat([start, tokenized_sentences, end], axis=1)\n","\n","    def cleanup_text(reserved_tokens, token_txt):\n","        \"\"\"\n","        Fonction qui nettoie un texte généré par la fonction detokenize() du BertTokenizer.\n","        Args :\n","            - reserved_tokens : Jetons réservés du BertTokenizer\n","            - token_text : Chaine généré par la fonction detokenize()\n","\n","        Returns :\n","            Texte nettoyé\n","        \"\"\"\n","        bad_tokens = [re.escape(token) for token in reserved_tokens if token != \"[UNK]\"] # Returns all non-[UNK] tokens with non-alphabetical characters backslashed\n","        bad_token_regex = \"|\".join(bad_tokens)\n","        bad_cells = tf.strings.regex_full_match(token_txt, bad_token_regex)\n","        return tf.strings.reduce_join(tf.ragged.boolean_mask(token_txt, ~bad_cells), separator=' ', axis=-1)\n"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1698289660742,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"ZooNRJnt-8s1","outputId":"93745f0a-0fd4-463e-d931-e078528ee8d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 320, 24, ..., 23, 21, 3], [2, 43, 45, 102, 30, 3]]\n"]}],"source":["def test_add_start_end():\n","\n","    tokenized_sentence = tf.ragged.constant([[320, 24, 500, 23, 21], [43, 45, 102, 30]], dtype=tf.int64)\n","    tf.print(LanguageTokenizer.add_start_end(tokenized_sentence))\n","\n","test_add_start_end()"]},{"cell_type":"markdown","metadata":{"id":"lT2uTmim-8s1"},"source":["Sortie attendue :\n","```\n","[[2, 320, 24, ..., 23, 21, 3], [2, 43, 45, 102, 30, 3]]\n","```"]},{"cell_type":"code","execution_count":135,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2252,"status":"ok","timestamp":1698289662989,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"VtOSl2dw-8s1","outputId":"3228aacf-d58d-4f69-c50a-7638ad452482"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary :  [PAD] [UNK] [START] [END] . ? a b d e h i k m n o p r s t u w y ##. ##? ##a ##b ##d ##e ##h ##i ##k ##m ##n ##o ##p ##r ##s ##t ##u ##w ##y\n","Tokenized sentence : <tf.RaggedTensor [[2, 10, 34, 40, 13, 25, 33, 41, 20, 4, 18, 16, 36, 28, 37, 30, 27, 28,\n","  33, 38, 37, 21, 28, 36, 28, 7, 34, 36, 33, 11, 33, 14, 28, 40, 22, 34,\n","  36, 31, 5, 3]]>\n","Detokenized sentence : how many u . s presidents were born in new york ?\n"]}],"source":["def test_tokenizer():\n","\n","    sentence = ['how many U.S Presidents were born in New York ?']\n","    vocab_path = root + 'test_language_vocab.txt'\n","    LanguageTokenizer.create_vocab(sentence, vocab_path)\n","\n","    with open(vocab_path) as f:\n","        vocab = f.read()\n","\n","    print('Vocabulary : ', vocab.replace('\\n', ' '))\n","    test_tokenizer_obj = LanguageTokenizer(LanguageTokenizer.reserved_tokens, vocab_path)\n","    tokenized_sentence = test_tokenizer_obj.tokenize(sentence)\n","    tf.print(f'Tokenized sentence : {tokenized_sentence}')\n","\n","    detokenized_sentence = test_tokenizer_obj.detokenize(tokenized_sentence)\n","    tf.print(f'Detokenized sentence : {bytes(tf.squeeze(detokenized_sentence).numpy()).decode()}')\n","\n","test_tokenizer()"]},{"cell_type":"markdown","metadata":{"id":"6cKfvxuC-8s1"},"source":["Sortie attendue :\n","```\n","Vocabulary :  [PAD] [UNK] [START] [END] . ? a b d e h i k m n o p r s t u w y ##. ##? ##a ##b ##d ##e ##h ##i ##k ##m ##n ##o ##p ##r ##s ##t ##u ##w ##y\n","Tokenized sentence : <tf.RaggedTensor [[2, 10, 34, 40, 13, 25, 33, 41, 20, 4, 18, 16, 36, 28, 37, 30, 27, 28,\n","  33, 38, 37, 21, 28, 36, 28, 7, 34, 36, 33, 11, 33, 14, 28, 40, 22, 34,\n","  36, 31, 5, 3]]>\n","Detokenized sentence : how many u . s presidents were born in new york ?\n","```"]},{"cell_type":"markdown","metadata":{"id":"PfNaakSS-8s1"},"source":["#### 2.1 Vocabulaire  (5 points)\n","\n","Vous pouvez maintenant créer le vocabulaire de chaque langage à l'aide de la fonction `create_vocab`. Vous pouvez stocker le vocabulaire anglais dans un fichier appelé `language_vocab_english.txt` et le vocabulaire sparql dans un fichier appelé `language_vocab_sparql.txt`"]},{"cell_type":"code","execution_count":136,"metadata":{"id":"55klqAgxCH_Q","executionInfo":{"status":"ok","timestamp":1698289721275,"user_tz":240,"elapsed":58291,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["LanguageTokenizer.create_vocab(processed_train['english'], root + 'language_vocab_english.txt')\n","LanguageTokenizer.create_vocab(processed_train['sparql'], root + 'language_vocab_sparql.txt')"]},{"cell_type":"markdown","metadata":{"id":"yq2b3NU1-8s1"},"source":["Afin de n'utiliser qu'une seule classe, nous allons créer une classe qui regroupe les deux tokenizers en une seule classe appelée `GroupedTokenizers`. Complétez le constructeur qui initialise l'attribut `english` correspondant au tokenizer anglais et l'attribut `sparql` correspondant au tokenizer sparql."]},{"cell_type":"code","execution_count":137,"metadata":{"id":"eza5AHd_C054","executionInfo":{"status":"ok","timestamp":1698289721275,"user_tz":240,"elapsed":11,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class GroupedTokenizers(tf.Module):\n","    \"\"\"\n","    Cette classe regroupe les deux segmenteurs (tokenizers) qui seront\n","    utilisés (une pour chacun des langages)\n","    \"\"\"\n","\n","    def __init__(self, reserved_tokens, vocab_english_path: str, vocab_sparql_path: str):\n","        \"\"\"\n","        Initialise les deux tokenizers (english and sparql)\n","        Args :\n","            - reserved_tokens : Jetons réservés du BertTokenizer\n","            - vocab_english_path : Chemin vers le fichier contenant\n","            le vocabulaire anglais du segmenteur (tokenizer)\n","            - vocab_sparql_path : Chemin vers le fichier contenant\n","            le vocabulaire sparql du segmenteur (tokenizer)\n","        \"\"\"\n","        self.english= LanguageTokenizer(reserved_tokens, vocab_english_path)\n","        self.sparql= LanguageTokenizer(reserved_tokens, vocab_sparql_path)"]},{"cell_type":"markdown","metadata":{"id":"629XMcxc-8s2"},"source":["Le test suivant permet de vérifier que votre pré-traitement et votre tokenizer fonctionnent correctement"]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2872,"status":"ok","timestamp":1698289724137,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"BVLoJCRLDhVX","outputId":"ad8541be-9b7e-499b-a5ba-2d9845e822b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["English : \n"," how many movies are there whose dbo:director is dbr:Stanley_Kubrick ? \n","\n","Processed english : \n"," how many movies are there whose dbo_director is dbr_Stanley_Kubrick \n","\n","Tokenized english : \n"," <tf.RaggedTensor [[2, 74, 75, 495, 67, 73, 65, 61, 25, 228, 59, 60, 25, 896, 95, 261, 25,\n","  36, 116, 329, 757, 114, 3]]> \n","\n","Detokenized english : \n"," how many movies are there whose dbo:director is dbr:stanley_kubrick \n","\n","\n","------------------------------------------------\n","\n","Sparql : \n"," select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . } \n","\n","Processed sparql : \n"," select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close \n","\n","Tokenized sparql : \n"," <tf.RaggedTensor [[2, 66, 65, 71, 68, 22, 63, 55, 22, 56, 68, 22, 62, 64, 57, 22, 63, 55,\n","  22, 56, 61, 22, 208, 59, 22, 41, 182, 80, 233, 22, 33, 879, 703, 111,\n","  60, 22, 58, 57, 22, 62, 3]]> \n","\n"]}],"source":["tokenizers = GroupedTokenizers(\n","    LanguageTokenizer.reserved_tokens,\n","    root + 'language_vocab_english.txt',\n","    root + 'language_vocab_sparql.txt'\n",")\n","\n","def test_tokenizer_preprocessor(tokenizers: GroupedTokenizers):\n","    \"\"\"\n","    Verifie que les fonctions du tokenizer et du preprocessor sont belles\n","    et bien codées. Si elles le sont, les phrases initiales anglaises et\n","    sparql devraient être identiques à celles en entrée\n","\n","    \"\"\"\n","    english = 'how many movies are there whose dbo:director is dbr:Stanley_Kubrick ?'\n","    sparql = 'select distinct count ( ?uri ) where { ?uri dbo:director dbr:Stanley_Kubrick . }'\n","    print('English : \\n', english, '\\n')\n","\n","    # processed_train = pre_processor.transform_dataframe\n","    pre_processor = Preprocessor()\n","    processed_english = pre_processor.transform_english(english)\n","    processed_sparql = pre_processor.transform_sparql(sparql)\n","\n","    print('Processed english : \\n', processed_english, '\\n')\n","    tokenized_english = tokenizers.english.tokenize(processed_english)\n","    print('Tokenized english : \\n', tokenized_english, '\\n')\n","    detokenized_english = pd.Series(tokenizers.english.detokenize(tokenized_english).numpy())\n","    print('Detokenized english : \\n', detokenized_english.apply(pre_processor.transform_back_english)[0], '\\n')\n","    print()\n","    print('------------------------------------------------')\n","    print()\n","\n","    print('Sparql : \\n', sparql, '\\n')\n","\n","    print('Processed sparql : \\n', processed_sparql, '\\n')\n","    tokenized_sparql = tokenizers.sparql.tokenize(processed_sparql)\n","    print('Tokenized sparql : \\n', tokenized_sparql, '\\n')\n","\n","test_tokenizer_preprocessor(tokenizers)"]},{"cell_type":"markdown","metadata":{"id":"njcViPKN1ndG"},"source":["### 3. Création de lots (Batching) (5 points)\n","\n","Étant donnée la grande quantité de données impliquant l'entrainement d'un modèle, il est important d'envoyer les données de la manière la plus efficace possible. Pour cela, les données sont regroupées en petits groupes appelés \"batchs\" ou lots. Cela permet notamment de traiter plusieurs éléments en parallèle et réduit considérablement le temps d'entrainement.\n","\n","Pour cela, la classe `Batcher` sera utilisée. Cette classe s'occupe de regrouper les données en petits lots et de les préparer pour les envoyer au modèle. Cette classe possède plusieurs fonctions :\n","- `make_batches`: Elle reçoit en paramètre une instance de la classe `tf.Dataset`. Elle divise ensuite le dataset en petits lots et les envoie à la fonction `prepare_batch`\n","- `prepare_batch`: Reçoit un lot/\"batch\" et le prépare en effectuant les transformations suivantes :\n","  - Segmente les phrases en entrée en utilisant les bons tokenizers passés en paramètres dans le constructeur\n","  - S'assure que la taille des phrases ne dépasse pas `max_tokens`\n","\n","\n","<img src=\"Batcher.png\" alt=\"Batcher\" width=\"100%\" height=\"700\"/>"]},{"cell_type":"code","execution_count":139,"metadata":{"id":"y0jovaYiEdh8","executionInfo":{"status":"ok","timestamp":1698289724137,"user_tz":240,"elapsed":4,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Batcher():\n","    \"\"\"\n","    Cette classe s'occupe de regrouper les données en petits groupes (batches) et\n","    de préparer les données pour les envoyer au modèle.\n","    \"\"\"\n","\n","    def __init__(self, tokenizers: GroupedTokenizers, train, max_tokens, batch_size, buffer_size):\n","        \"\"\"\n","        Initialise les paramètres en entrée\n","\n","        Args :\n","            - tokenizers : tokenizers pour transformer les entrées en jeton\n","            - train : Valeur booléenne pour savoir si les batches seront utilisées\n","            pour de l'entrainement ou pas\n","            - max_tokens : Nombre de jetons maximums pour une entrée\n","            - batch_size : Taille des groupes (batches)\n","            - buffer_size : Taille du buffer servant à mélanger les données dans le\n","            cas de l'entrainement\n","        \"\"\"\n","\n","        self.tokenizers = tokenizers\n","        self.train = train\n","        self.max_tokens = max_tokens\n","        self.buffer_size = buffer_size\n","        self.batch_size = batch_size\n","\n","    def prepare_batch(self, input_language, output_language=None):\n","        \"\"\"\n","        Prépare les batches pour les envoyer au modèle. Cette fonction est\n","        appelée pour chaque élément d'un Tensorflow Dataset.\n","\n","        Effectue les transformations suivantes :\n","            - Tokenize les phrases en entrées en utilisant les bons tokenizers passés\n","            en paramètre dans le constructeur\n","            - S'assure que la taille des phrases ne dépasse pas `max_tokens` (max_tokens\n","            est inclus)\n","\n","        Args :\n","            - input_language : Entrée dans le langage d'entrée (sparql dans notre cas)\n","            de la taille (self.batch_size, x)\n","            - output_language : Sortie dans le langage de sortie (english dans notre cas)\n","            de la taille (self.batch_size, x). None dans le cas de batches de test\n","\n","        Returns :\n","            - Si self.train == True :\n","                Retourne un tuple de la forme ((input_langage, output_language_inputs), output_language_labels)\n","                qui seront les entrées respectives de l'encodeur et du décodeur et la\n","                sortie du décodeur.\n","\n","                Voici ce que chaque valeur de retour représente\n","                - input_language : tenseur contenant les jetons du paramètre `input_language`\n","                limité à `max_tokens`\n","                - output_language_inputs : tenseur contenant les jetons du paramètre\n","                `output_language` limité à `max_tokens`+1 (pour permettre de prédire le prochain\n","                jeton)\n","                - output_language_labels : tenseur contenant les jetons du paramètre\n","                `output_language` contenant le prochain charactère\n","\n","            - Si self.train == False :\n","                Retourne un tuple de la forme (input_language, output_language) qui\n","                représentent l'entrée de l'encodeur et un\n","                tenseur de sortie initialisé avec le jeton d'entrée de la taille\n","                (self.batch_size,). Les valeurs de retour sont expliquées plus haut\n","        \"\"\"\n","        if self.train:\n","            # On tokenize les phrases en entrée et en sortie\n","            input_language = self.tokenizers.sparql.tokenize(input_language)\n","            output_language = self.tokenizers.english.tokenize(output_language)\n","\n","            # On s'assure que la taille des phrases ne dépasse pas max_tokens\n","            input_language = input_language[:, :self.max_tokens]\n","            output_language = output_language[:, :self.max_tokens+1]\n","\n","            input_language = input_language.to_tensor()\n","\n","            # On sépare les sorties en entrée et en label\n","            output_language_inputs = output_language[:, :-1].to_tensor()\n","            output_language_labels = output_language[:, 1:].to_tensor()\n","\n","            return (input_language, output_language_inputs), output_language_labels\n","\n","        else:\n","            # On tokenize les phrases en entrée\n","            input_language_tokenized = self.tokenizers.sparql.tokenize(input_language)\n","\n","            # On s'assure que la taille des phrases ne dépasse pas max_tokens\n","            input_language_tokenized = input_language_tokenized[:, :self.max_tokens].to_tensor()\n","\n","            # On initialise la sortie avec le jeton d'entrée\n","            output_language = tf.fill([len(input_language)], self.tokenizers.english.START)\n","            output_language = tf.expand_dims(output_language, axis=1)\n","\n","\n","            return input_language_tokenized, output_language\n","\n","\n","    def make_batches(self, ds):\n","        \"\"\"\n","        Args :\n","            - ds : Dataset contenant les examples de la forme\n","            ((sparql, english_in), english_label)\n","            si self.train == True et de la forme (sparql, english)\n","            si self.train == False\n","\n","        Returns :\n","            Le dataset initial (mélangé si self.train == True) contenant des\n","            éléments de la taille de self.batch_size dont la fonction self.prepare_batch\n","            a été appelée sur chacun des éléments et dont les éléments sont\n","            pré-récupéré (prefetched). Si self.train == False, c'est le même principe,\n","            mais les données ne sont pas mélangées\n","        \"\"\"\n","        # ds est de type zipDataset, on suppose que les phrases sont déjà passées par le pre_processor\n","\n","        # On melange les donnees si self.train == True\n","        if self.train:\n","            ds = ds.shuffle(self.buffer_size)\n","        # On separe les donnees en batches de taille batch_size\n","        ds = ds.batch(self.batch_size)\n","        # On le map pour appliquer la fonction prepare_batch sur chaque element\n","        ds = ds.map(self.prepare_batch, tf.data.experimental.AUTOTUNE)\n","        # On prefetch les donnees\n","        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","        return ds"]},{"cell_type":"markdown","metadata":{"id":"KlL-Jmg0-8s2"},"source":["Vous pouvez maintenant tester le batcher à l'aide de la fonction suivante (vérifiez bien que la sortie du décodeur contient un jeton de plus que la phrase qui entre dans le décodeur et que ce qui rentre dans l'encodeur est bel et bien du sparql)."]},{"cell_type":"code","execution_count":140,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7788,"status":"ok","timestamp":1698289731921,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"8UUXIBuS-8s2","outputId":"cee1216e-693a-492a-c20c-a64e02ed7327"},"outputs":[{"output_type":"stream","name":"stdout","text":["Detokenized inputs encoder :  [\"select distinct var _ uri where brack\" \"select distinct count parent _ open var\"]\n","Detokenized inputs decoder  :  [\"what is the dbo _ river whose\" \"how many movies are there whose dbo\"]\n","Detokenized outputs decoder :  [\"what is the dbo _ river whose dbo\" \"how many movies are there whose dbo _\"]\n","Concatened values :  tf.Tensor(\n","[[  2  66  65  55  22  56  64  57   2  64  59  58  61  25  97  65  64  59\n","   58  61  25  97  65  61]\n"," [  2  66  65  71  68  22  63  55   2  74  75 495  67  73  65  61  74  75\n","  495  67  73  65  61  25]], shape=(2, 24), dtype=int64)\n"]}],"source":["def test_batcher(tokenizers):\n","\n","\n","    english = pd.Series([\n","        'how many movies are there whose dbo_director is dbr_Stanley_Kubrick',\n","        'what is the dbo_River whose dbo_riverMouth is dbr_Dead_Sea',\n","    ])\n","\n","    sparql = pd.Series([\n","        'select distinct count parent_open var_uri parent_close where brack_open var_uri dbo_director dbr_Stanley_Kubrick sep_dot brack_close',\n","        'select distinct var_uri where brack_open var_uri dbo_riverMouth dbr_Dead_Sea sep_dot var_uri rdf_type dbo_River brack_close',\n","    ])\n","\n","    batcher = Batcher(tokenizers, True, 8, 64, 20000)\n","\n","    val_english = tf.data.Dataset.from_tensor_slices(english)\n","    val_sparql = tf.data.Dataset.from_tensor_slices(sparql)\n","    val_examples = tf.data.Dataset.zip((val_sparql, val_english))\n","\n","    batches = batcher.make_batches(val_examples)\n","    for x in batches:\n","        tf.print('Detokenized inputs encoder : ', tokenizers.sparql.detokenize(x[0][0]))\n","        tf.print('Detokenized inputs decoder  : ', tokenizers.english.detokenize(x[0][1]))\n","        tf.print('Detokenized outputs decoder : ', tokenizers.english.detokenize(x[1]))\n","\n","        concat = tf.concat([x[0][0], x[0][1], x[1]], axis=1)\n","        print('Concatened values : ', concat)\n","\n","test_batcher(tokenizers)"]},{"cell_type":"markdown","metadata":{"id":"8Uy4egm3N_py"},"source":["### 4. Transformer (30 points)\n","\n","<img style=\"float: right;\" src=\"Transformer.png\" alt=\"Transformer\" width=\"500\" height=\"700\"/>\n","\n","\n","Maintenant que les données sont prêtes à être envoyées au modèle, il ne manque qu'à créer son architecture. Pour cela, la librairie Keras sera utilisée. Keras est une librairie qui est construite au dessus de Tensorflow pour faciliter le développement de modèles dans un style orienté objet. Depuis Tensorflow 2.0, elle est maintenant directement intégrée à Tensorflow. Pour plus de détails, la documentation est présente sur ce [site](https://keras.io/api/)\n","\n","L'architecture qui sera suivie dans ce TP est présentée dans l'image à droite. La liste des couches qui seront implémentées sont les suivantes :\n","- `Positional Embedding` : Permet la génération des plongements de position\n","- `Global-Self Attention` : S'occupe du mécanisme d'attention de l'encodeur\n","- `Feed Forward` : Permet de connecter des entrées et des sorties avec un réseau de neurones\n","- `Decoder Attention` : S'occupe du premier mécanisme d'attention du décodeur\n","- `Cross Attention` : S'occupe du deuxième mécanisme d'attention du décodeur (relie l'encodeur au décodeur)\n","\n","Les couches d'addition et de normalisation seront incluses dans les couches précédentes. Par exemple, la couche `Add & Norm` qui suit la couche `Global-Self Attention` dans le graphique sera inclus dans la couche `Global-Self Attention`.\n","\n","Ensuite, des couches seront égalements utilisées pour regrouper ces couches pour simplifier le pipeline du Transformer. Voici la liste des couches qui seront ajoutées à celles sur le graphique :\n","- `Encoder Layer` : Représente un seul encodeur contenant les couches `Global-Self Attention` et `Feed Forward`\n","- `Decoder Layer` : Représente un seul décodeur contenant les couches `Decoder Attention`, `Cross Attention` et `Feed Forward`\n","- `Encoder` : Représente plusieurs encodeurs en parallèle\n","- `Decoder` : Représente plusieurs décodeurs en parallèle\n","- `Transformer` : Représente le Transformer au complet et regroupe tous les encodeurs, décodeurs et les couches de plongements\n","\n","Chaque couche sera créée manuellement et implémentée en tant que couche Keras. Si vous n'êtes pas familier avec Keras, voici quelques tutoriels qui pourraient vous aider :\n","- https://keras.io/api/models/model/\n","- https://www.tensorflow.org/text/tutorials/transformer\n","- https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/\n","\n","\n","Les classes sont déjà créées pour vous et vous n'aurez principalement qu'à implémenter la fonction `call()` de chacune de ces classes. La fonction `call()` s'occupe, pour une couche donnée, de transformer une entrée en sortie."]},{"cell_type":"markdown","metadata":{"id":"0Z4VEorxV_DL"},"source":["#### 4.1 Positional Embedding  \n","\n","Pour permettre au modèle de prendre en compte l'ordre des jetons qui lui sont passés, il est important de passer de l'information au modèle à propos de la position des jetons dans une phrase. C'est la couche de `PositionalEmbedding` qui s'occupe de cela. À l'aide de la formule suivante, des plongements de position sont générés, ce qui permet d'incorporer la position d'un jeton dans son plongement :\n","$$PE_{(pos, 2i)} = sin \\Big( \\frac{pos}{10000^{2i/d_{model}}} \\Big)$$\n","$$PE_{(pos, 2i+1)} = cos \\Big( \\frac{pos}{10000^{2i/d_{model}}} \\Big)$$\n","\n","où $d_{model}$ est la dimension des plongements de sortie et $i$ est simplement l'indice d'une valeur dans le vecteur de plongement.\n","\n","La fonction `generate_positional_embedding` qui génère les plongements de position vous est fournie. Celle-ci prend en entrée :\n","- `length` : Nombre de jetons maximal dont on doit générer le plongement de position\n","- `depth` : Dimension des plongements du modèle.\n","\n","La fonction `call` de cette couche est appelée avec le paramètre suivant (les tailles des tenseurs sont indiquées entre parenthèses) :\n","- `x` (de taille [batch_size, input_size] où le batch_size est le nombre d'éléments qui sont envoyés à la fois pour une itération de l'entraînement et input_size est la taille maximale des phrases en entrée) : Entrées de la couche. Cela correspond notamment au tenseur contenant les indices de chaque jeton correspondant à la phrase\n","  \n","\n","Elle retourne le plongement de l'entrée dans l'espace latent incluant les positions des jetons (batch_size, input_size, dim_model).\n","\n","La fonction `call` doit effectuer les opérations suivantes :\n","1. Appeler la couche `embedding_layer` qui génère des plongements par rapport aux entrées\n","2. Multiplier chaque valeur par la racine de `dim_model` (Cette multiplication sert à agrandir les plongements pour qu'ils soient d'un ordre de grandeur comparable aux plongements de position qui sont ajoutés par la suite. Pour plus de détails, consultez l'article original ayant mené à la création du Transformer intitulé \"Attention Is All You Need\").\n","3. Ajouter ensuite les plongements de positions aux plongements générés par la couche `embedding_layer` (après qu'ils aient été multipliés par la racine de `dim_model`)"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"czAWrftUdkvq","executionInfo":{"status":"ok","timestamp":1698289731922,"user_tz":240,"elapsed":4,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    \"\"\"\n","    Classe représentant l'étape qui incorpore dans l'espace latent les positions des jetons\n","    \"\"\"\n","    def __init__(self, input_size, dim_model):\n","        \"\"\"\n","        Initialise une couche de plongements et les plongements de position\n","\n","        Args :\n","            - input_size : Taille d'entrée de la couche (taille du vocabulaire)\n","            - dim_model : Taille des plongements du modèle (taille du plongement de sortie de la couche)\n","        \"\"\"\n","        super().__init__()\n","        self.embedding_layer = tf.keras.layers.Embedding(input_size, dim_model, mask_zero=True)\n","        self.position_embeddings = self.generate_positions_embedding(length=2048, depth=dim_model)\n","        self.dim_model = dim_model\n","\n","    def compute_mask(self, *args, **kwargs):\n","        return self.embedding_layer.compute_mask(*args, **kwargs)\n","\n","    def generate_positions_embedding(self, length, depth):\n","        depth = depth/2\n","\n","        positions = np.arange(length)[:, np.newaxis]\n","        depths = np.arange(depth)[np.newaxis, :]/depth\n","\n","        angle_rates = 1 / (10000**depths)\n","        angle_rads = positions * angle_rates\n","\n","        pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n","\n","        return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","    def call(self, x):\n","        \"\"\"\n","        Exécute la couche de plongements sur l'entrée en la normalisant sur la racine de la dimension de sortie\n","        \"\"\"\n","        x = self.embedding_layer(x) * tf.math.sqrt(tf.cast(self.dim_model, tf.float32))\n","        x = x + self.position_embeddings[tf.newaxis, :tf.shape(x)[1], :]\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"M_OlPgAXYMJv"},"source":["#### 4.2 Attention  (15 points)\n","\n","Les couches d'attention reposent toutes sur la même base qui contient une tête d'attention multiple, une couche de normalisation et une couche d'addition. La seule différence entre les différentes couches d'attention sont les entrées `Q` (query), `K` (key), et `V` (value) qui seront envoyées à la formule :\n","\n","$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","Pour cela, la classe `DefaultAttention`, une classe dont toutes les autres couches d'attention hériteront, a été créée pour éviter de répéter 3 fois le même constructeur. Vous devez compléter les fonctions `call()` de chacune des sous-classes, soit `CrossAttention`, `GlobalSelfAttention` et `DecoderAttention`. Pour évaluer les valeurs de `K`, `V` et `Q` de chaque couche d'attention, référez-vous au graphique de l'architecture.\n"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"tcAX1Ixr-8s4","executionInfo":{"status":"ok","timestamp":1698289731922,"user_tz":240,"elapsed":3,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class DefaultAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    Couche d'attention de base contenant des têtes d'attention suivies d'une couche de normalisation et d'addition\n","    \"\"\"\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","        self.multiHeadAttention = tf.keras.layers.MultiHeadAttention(**kwargs)\n","        self.layerNormalization = tf.keras.layers.LayerNormalization()\n","        self.addLayer = tf.keras.layers.Add()\n"]},{"cell_type":"markdown","metadata":{"id":"PV9zjpb--8s4"},"source":["\n","##### 4.2.1 CrossAttention (5 points)\n","Dans le cas de la couche `CrossAttention`, la fonction `call` prend en paramètres les entrées suivantes :\n","- `input` : Les entrées de la couche, correspondant à la sortie de la couche `DecoderAttention`\n","- `context` : La sortie de l'encodeur\n","- `training` : Valeur booléenne indiquant si le modèle est en entraînement ou pas.\n","\n","Cette fonction doit exécuter les opérations suivantes :\n","1. Appliquer la couche de têtes d'attention multiples avec les bonnes valeurs de `K`, `V` et `Q` (Ne pas oublier de passer l'argument `training` à la couche).\n","2. Ajouter la sortie de la couche de tête d'attention aux entrées à l'aide de la couche `Add`\n","3. Normaliser le tout à l'aide de la couche de normalisation"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"w1KuFLwH-8s4","executionInfo":{"status":"ok","timestamp":1698289731922,"user_tz":240,"elapsed":3,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class CrossAttention(DefaultAttention):\n","    \"\"\"\n","    Couche qui connecte l'encodeur au décodeur.\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        \"\"\"\n","        Initialise une couche de têtes d'attention suivie d'une couche de normalisation\n","        puis d'addition\n","        \"\"\"\n","        super().__init__(**kwargs)\n","\n","    def call(self, input, context, training):\n","        \"\"\"\n","        Exécute la couche d'attention. Ajoute les sorties d'attention à l'entrée et\n","        normalise le tout\n","        \"\"\"\n","        attention = self.multiHeadAttention(input, context, context, training=training)\n","        attention = self.addLayer([input, attention])\n","        attention = self.layerNormalization(attention)\n","        return attention\n"]},{"cell_type":"markdown","metadata":{"id":"UirGoNDP-8s4"},"source":["\n","##### 4.2.2 GlobalSelfAttention  (5 points)\n","Dans le cas de la couche `GlobalSelfAttention`, la fonction `call` prend en paramètres les entrée suivantes :\n","- `input` : Les entrées de la couche, correspondant à la sortie de la couche `DecoderAttention`\n","- `training` : Valeur booléenne indiquant si le modèle est en entraînement ou pas.\n","\n","Cette fonction doit exécuter les opérations suivantes :\n","1. Appliquer la couche de têtes d'attention multiples avec les bonnes valeurs de `K`, `V` et `Q` (Ne pas oublier de passer l'argument `training` à la couche).\n","2. Ajouter la sortie de la couche de tête d'attention aux entrées à l'aide de la couche `Add`\n","3. Normaliser le tout à l'aide de la couche de normalisation"]},{"cell_type":"code","execution_count":144,"metadata":{"id":"znHipfCO-8s4","executionInfo":{"status":"ok","timestamp":1698289731922,"user_tz":240,"elapsed":3,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class GlobalSelfAttention(DefaultAttention):\n","    \"\"\"\n","    Couch d'auto-attention permettant au modèle de regarder les autres mots de\n","    la phrase d'entrée lorsqu'il encode un mot spécifique\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        \"\"\"\n","        Initialise une couche de têtes d'attention suivie d'une couche de\n","        normalisation puis d'addition\n","        \"\"\"\n","        super().__init__(**kwargs)\n","\n","    def call(self, input, training):\n","        \"\"\"\n","        Exécute la couche d'attention. Ajoute les sorties d'attention à l'entrée\n","        et normalise le tout\n","        \"\"\"\n","        attention = self.multiHeadAttention(input, input, input, training=training)\n","        attention = self.addLayer([input, attention])\n","        attention = self.layerNormalization(attention)\n","        return attention\n"]},{"cell_type":"markdown","metadata":{"id":"QGptvUHH-8s4"},"source":["##### 4.2.3 DecoderAttention  (5 points)\n","Dans le cas de la couche `DecoderAttention`, la fonction `call` prend en paramètres les entrées suivantes :\n","- `input` : Les entrées de la couche, correspondant à la sortie de la couche `DecoderAttention`\n","- `training` : Valeur booléenne indiquant si le modèle est en entraînement ou pas.\n","\n","L'implémentation de la méthode est très similaire à la fonction `call` de la classe `GlobalSelfAttention`, mais diffère en un point clé : le masque causal. Ce masque permet notamment de ne pas considérer les jetons futurs lorsque le mécanisme d'attention est calculé. Cela évite au Transformer de s'entraîner en connaissant les jetons futurs qu'il doit prédire (donc en \"trichant\"). Cet [article](https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c) donne plus d'information sur le masque causal.\n","\n","Cette fonction doit exécuter les opérations suivantes :\n","1. Appliquer la couche de têtes d'attention multiples avec les bonnes valeurs de `K`, `V` et `Q` (Ne pas oublier de passer l'argument `training` à la couche et d'activer le masque causal de la couche en mettant l'attribut `use_causal_mask` à `True` lors de l'appel de la couche d'attention).\n","2. Ajouter la sortie de la couche de tête d'attention aux entrées à l'aide de la couche `Add`\n","3. Normaliser le tout à l'aide de la couche de normalisation"]},{"cell_type":"code","execution_count":145,"metadata":{"id":"o7UdIeC7XBUz","executionInfo":{"status":"ok","timestamp":1698289731922,"user_tz":240,"elapsed":3,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class DecoderAttention(DefaultAttention):\n","    \"\"\"\n","    Couche d'attention semblable à la couche globale d'auto-attention, mais en masquant\n","    les données qui viennent après\n","    \"\"\"\n","    def __init__(self, **kwargs):\n","        \"\"\"\n","        Initialise une couche de têtes d'attention suivie d'une couche de normalisation\n","        puis d'addition\n","        \"\"\"\n","        super().__init__(**kwargs)\n","\n","    def call(self, input, training):\n","        \"\"\"\n","        Exécute la couche d'attention en masquant les données après. Ajoute les sorties\n","        d'attention à l'entrée et normalise le tout\n","        \"\"\"\n","        attention = self.multiHeadAttention(input, input, input, training=training, use_causal_mask=True)\n","        attention = self.addLayer([input, attention])\n","        attention = self.layerNormalization(attention)\n","        return attention"]},{"cell_type":"markdown","metadata":{"id":"qw7KLOpN-8s5"},"source":["Vous pouvez tester votre implémentation des couches d'attention à l'aide de la fonction suivante :"]},{"cell_type":"code","execution_count":146,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698289732200,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"dmM11qkM-8s5","outputId":"05c676bf-c3b2-425d-ce39-53446c5fe9d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cross Attention result : \n","tf.Tensor(\n","[[[ 124 -119   -4]]\n","\n"," [[ 140  -59  -80]]\n","\n"," [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n","\n","Global-Self Attention result : \n","tf.Tensor(\n","[[[ 124 -119   -4]]\n","\n"," [[ 140  -59  -80]]\n","\n"," [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n","\n","Decoder Attention result : \n","tf.Tensor(\n","[[[ 124 -119   -4]]\n","\n"," [[ 140  -59  -80]]\n","\n"," [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32) \n","\n"]}],"source":["def test_attention():\n","    config = {\n","        'num_heads': 3,\n","        'key_dim': 3,\n","        'dropout': 0.1\n","    }\n","    cross_attention = CrossAttention(**config)\n","    global_self_attention = GlobalSelfAttention(**config)\n","    decoder_attention = DecoderAttention(**config)\n","\n","    # Create determinisitc inputs and context\n","    generator = tf.random.Generator.from_seed(1)\n","    input = generator.normal(shape=(3, 1, 3))\n","    context = generator.normal(shape=(3, 1, 3))\n","\n","    # Make attention layer deterministic\n","    layer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=4, dropout=0.1, kernel_initializer=tf.keras.initializers.ones())\n","    cross_attention.multiHeadAttention = layer\n","    global_self_attention.multiHeadAttention = layer\n","    decoder_attention.multiHeadAttention = layer\n","\n","    outputs_cross_attention = tf.cast(cross_attention(input, context) * 100, tf.int32)\n","    outputs_global_self_attention = tf.cast(global_self_attention(input) * 100, tf.int32)\n","    outputs_decoder_attention = tf.cast(decoder_attention(input) * 100, tf.int32)\n","\n","    print('Cross Attention result : ')\n","    print(outputs_cross_attention, '\\n')\n","\n","    print('Global-Self Attention result : ')\n","    print(outputs_global_self_attention, '\\n')\n","\n","    print('Decoder Attention result : ')\n","    print(outputs_decoder_attention, '\\n')\n","\n","test_attention()"]},{"cell_type":"markdown","metadata":{"id":"-W9raVHb-8s5"},"source":["Sortie attendue :\n","\n","```\n","Cross Attention result :\n","tf.Tensor(\n","[[[ 124 -119   -4]]\n","\n"," [[ 140  -59  -80]]\n","\n"," [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32)\n","\n","Global-Self Attention result :\n","tf.Tensor(\n","[[[ 124 -119   -4]]\n","\n"," [[ 140  -59  -80]]\n","\n"," [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32)\n","\n","Decoder Attention result :\n","tf.Tensor(\n","[[[ 124 -119   -4]]\n","\n"," [[ 140  -59  -80]]\n","\n"," [[  79   60 -140]]], shape=(3, 1, 3), dtype=int32)\n","```"]},{"cell_type":"markdown","metadata":{"id":"84vLhOVGY7vf"},"source":["#### 4.3 Feed Forward  (5 points)\n","\n","La couche Feed Forward est, dans notre cas, simplement une séquence de 2 couches denses, d'une couche de dropout, d'une couche d'addition et d'une couche de normalisation. Ces couches sont déjà initialisées dans le constructeur à l'aide d'une couche `Sequential` qui regroupe plusieurs couches et les applique une à la suite de l'autre.\n","\n","La fonction `call` prend en paramètres les entrées suivantes :\n","- `input` : Entrées de la couche (varie en fonction d'où est située cette couche dans l'architecture)\n","\n","Elle retourne ensuite le résultat une fois que les transformations sont appliquées sur les entrées\n","\n","Elle effectue les opérations suivantes :\n","1. Exécute la couche séquentielle initialisée dans le constructeur\n","2. Ajoute le résultat de la couche séquentielle aux entrées\n","3. Normalise le tout à l'aide de la couche de normalisation"]},{"cell_type":"code","execution_count":147,"metadata":{"id":"gm_4ethRYKR_","executionInfo":{"status":"ok","timestamp":1698289732200,"user_tz":240,"elapsed":3,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class FeedForward(tf.keras.layers.Layer):\n","    \"\"\"\n","    Couche de propagation à la sortie des couches d'attention\n","    \"\"\"\n","\n","    def __init__(self, dim_model, feed_forward_size, dropout_rate=0.1):\n","        \"\"\"\n","        Initialise des couches de propagation dense (avec dropout), d'addition et de normalisation\n","        Args :\n","            - dim_model : Dimension du modèle (sortie de la couche)\n","            - feed_forward_size : Dimension de la couche dense de propagation (entrée)\n","            - dropout_rate : Ratio des entrées de la couche de dropout qui\n","            seront initialisés à zéro de manière aléatoire\n","        \"\"\"\n","        super().__init__()\n","        self.seq = tf.keras.Sequential([\n","            tf.keras.layers.Dense(feed_forward_size, activation='relu'),\n","            tf.keras.layers.Dense(dim_model),\n","            tf.keras.layers.Dropout(dropout_rate)\n","        ])\n","        self.add = tf.keras.layers.Add()\n","        self.layer_norm = tf.keras.layers.LayerNormalization()\n","\n","    def call(self, input):\n","        \"\"\"\n","        Exécute les couches de propagation sur l'entrée, additionne le tout et normalise\n","        \"\"\"\n","        seq = self.seq(input)\n","        seq = self.add([input, seq])\n","        seq = self.layer_norm(seq)\n","        return seq"]},{"cell_type":"markdown","metadata":{"id":"CxPcLmhrZq-c"},"source":["#### 4.4 Encodeur (3 points)"]},{"cell_type":"markdown","metadata":{"id":"UQS6w5Ud-8s6"},"source":["L'encodeur de notre Transformer est composé en réalité de plusieurs couches appelées `EncoderLayer`. Ces couches représentent une seule passe d'un encodeur. Cependant, la classe `Encoder` regroupe plusieurs de ces `EncoderLayer` pour permettre au Transformer de capturer des contextes plus compliqués entre les mots.\n","\n","Vous aurez donc à compléter la méthode `call` de la classe `EncoderLayer`. Cette méthode prend en entrée les paramètres suivants :\n","- `input` : Entrées de la couche (notamment la sortie de la classe `PositionalEmbedding`)\n","- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n","\n","Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches (`GlobalSelfAttention`, `FeedForward`)\n","\n","Cette méthode devra exécuter les opérations suivantes :\n","1. Appeler la couche d'attention avec les entrées\n","2. Appeler la couche de propagation sur la sortie de la couche d'attention"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"IIyENqA5Y4za","executionInfo":{"status":"ok","timestamp":1698289732200,"user_tz":240,"elapsed":2,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Classe représentant une couche d'encodeur\n","    \"\"\"\n","\n","    def __init__(self, *, dim_model, num_heads, feed_forward_size, dropout_rate=0.1):\n","        \"\"\"\n","        Initialise une couche d'auto-attention suivie d'une couche de propagation\n","\n","        Args :\n","            dim_model : Dimension des embeddings du model\n","            num_heads : Nombre de têtes d'attention de l'encodeur\n","            feed_forward_size : Nombre de neurones du feed forward\n","            dropout_rate : Ratio des entrées de la couche d'attention qui seront\n","            initialisés à zéro de manière aléatoire\n","        \"\"\"\n","        super().__init__()\n","\n","        self.self_attention = GlobalSelfAttention(\n","            num_heads=num_heads,\n","            key_dim=dim_model,\n","            dropout=dropout_rate\n","        )\n","\n","        self.ffn = FeedForward(dim_model, feed_forward_size)\n","\n","    def call(self, input, training):\n","        \"\"\"\n","        Exécute la couche d'attention et de propagation sur les entrées.\n","        L'argument training spécifie si l'appel est effectué durant l'entrainement\n","        ou pas (important pour la couche d'attention)\n","        \"\"\"\n","        attention = self.self_attention(input, training=training)\n","        ffn = self.ffn(attention)\n","        return ffn"]},{"cell_type":"markdown","metadata":{"id":"sena7EN2-8s6"},"source":["Maintenant, la classe `Encoder` s'occupe de regrouper plusieurs `EncoderLayer` pour permettre au Transformer d'inférer des contextes plus complexes.\n","\n","La méthode `call` de la classe `Encoder` prend en entrée les paramètres suivants :\n","- `input` : Entrées de la couche (correspondant aux indices des jetons de la phrase)\n","- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n","\n","Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches d'encodeur\n","\n","Cette méthode exécute les opérations suivantes :\n","1. Appeler la couche de plongements de position sur les entrées\n","2. Appliquer la couche de dropout sur le résultat\n","3. Appeler toutes les couches `EncoderLayer` (la sortie d'une couche d'encodeur devient l'entrée d'une autre)   "]},{"cell_type":"code","execution_count":149,"metadata":{"id":"OoIdsp__ZjkE","executionInfo":{"status":"ok","timestamp":1698289732433,"user_tz":240,"elapsed":235,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","    \"\"\"\n","    Classe représentant tous les encodeurs du Transformer\n","    \"\"\"\n","\n","    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size, vocab_size, dropout_rate=0.1):\n","        \"\"\"\n","        Initialise la couche de plongements de position, une couche dropout et les couches d'encodeurs\n","        Args :\n","            num_layers : Nombre de couches d'encodeurs\n","            dim_model : Dimension des embeddings du model\n","            num_heads : Nombre de têtes d'attention de l'encodeur\n","            feed_forward_size : Dimension du feed forward (en sortie)\n","            vocab_size : Taille du vocabulaire (correspondant à la taille d'entrée de la\n","            couche de plongements de position)\n","            dropout_rate : Ratio des entrées de la couche de dropout qui seront initialisés\n","            à zéro de manière aléatoire\n","        \"\"\"\n","        super().__init__()\n","\n","        self.dim_model = dim_model\n","        self.num_layers = num_layers\n","\n","        self.pos_embedding = PositionalEmbedding(input_size=vocab_size, dim_model=dim_model)\n","\n","        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","        self.enc_layers = [EncoderLayer(dim_model=dim_model, num_heads=num_heads, feed_forward_size=feed_forward_size, dropout_rate=dropout_rate) for _ in range(num_layers)]\n","\n","    def call(self, input, training):\n","        \"\"\"\n","        Execute la couche de plongements et de dropout puis toutes les couches d'encodeurs\n","        \"\"\"\n","        input = self.dropout(self.pos_embedding(input))\n","        for i in range(self.num_layers):\n","            input = self.enc_layers[i](input, training)\n","\n","        return input\n"]},{"cell_type":"markdown","metadata":{"id":"4gL0Nvfwcf1x"},"source":["#### 4.5 Decodeur (3 points)"]},{"cell_type":"markdown","metadata":{"id":"M0uehnmF-8s6"},"source":["Le décodeur de notre Transformer est composé en réalité de plusieurs couches appelées `DecoderLayer`. Ces couches représentent une seule passe d'un décodeur. Cependant, la classe `Decoder` regroupe plusieurs de ces `DecoderLayer` pour permettre au Transformer de capturer des contextes plus compliqués entre les mots.\n","\n","Vous aurez donc à compléter la méthode `call` de la classe `DecoderLayer`. Cette méthode prend en entrée les paramètres suivants :\n","- `input` : Entrées de la couche\n","- `context` : Le contexte des couches d'attention\n","- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n","\n","Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches (`DecoderAttention`, `CrossAttention`, `FeedForward`)\n","\n","Cette méthode devra exécuter les opérations suivantes :\n","1. Appeler la couche d'attention du décodeur avec les entrées\n","2. Appeler la couche d'attention croisée\n","3. Appeler la couche de propagation (`FeedForward`)"]},{"cell_type":"code","execution_count":150,"metadata":{"id":"7t3-gOticXIr","executionInfo":{"status":"ok","timestamp":1698289732434,"user_tz":240,"elapsed":4,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    Classe représentant une couche de décodeur\n","    \"\"\"\n","\n","    def __init__(self, *, dim_model, num_heads, feed_forward_size, dropout_rate=0.1):\n","        \"\"\"\n","        Args :\n","            dim_model : Dimension des embeddings du model\n","            num_heads : Nombre de têtes d'attention du décodeur\n","            feed_forward_size : Nombre de neurones du feed forward\n","            dropout_rate : Ratio de dropout pour les neurones de la couche de Feed Forward\n","        \"\"\"\n","        super().__init__()\n","\n","        self.encoder_decoder_attention = DecoderAttention(\n","            num_heads=num_heads,\n","            key_dim=dim_model,\n","            dropout=dropout_rate\n","        )\n","\n","        self.cross_attention = CrossAttention(\n","            num_heads=num_heads,\n","            key_dim=dim_model,\n","            dropout=dropout_rate\n","        )\n","\n","        self.ffn = FeedForward(dim_model, feed_forward_size)\n","\n","    def call(self, input, context, training):\n","        \"\"\"\n","        Exécute les couches d'attention suivies des couches de propagation FFN\n","        \"\"\"\n","        attention = self.encoder_decoder_attention(input, training)\n","        cross_attention = self.cross_attention(attention, context, training)\n","        ffn = self.ffn(cross_attention)\n","        return ffn"]},{"cell_type":"markdown","metadata":{"id":"BfqSG2OH-8s7"},"source":["La classe `Decoder` s'occupe de regrouper plusieurs `DecoderLayer`.\n","\n","La méthode `call` de la classe `Decoder`prend en entrée les paramètres suivants :\n","- `input` : Entrées de la couche (correspondant aux indices des jetons de la phrase)\n","- `context` : Contexte des couches d'attention (correspondant à la sortie de l'encodeur)\n","- `training` : Valeur booléenne indiquant si la méthode est appelée durant l'entraînement ou pas\n","\n","Elle retourne les entrées une fois qu'elles sont passées à travers toutes les couches de décodeur\n","\n","Cette méthode  exécute les opérations suivantes :\n","1. Appeler la couche de plongements de position sur les entrées\n","2. Appliquer la couche de dropout sur le résultat\n","3. Appeler les couches `DecoderLayer` successivement\n"]},{"cell_type":"code","execution_count":151,"metadata":{"id":"NdzjzJ2wcwDu","executionInfo":{"status":"ok","timestamp":1698289732434,"user_tz":240,"elapsed":4,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size, vocab_size, dropout_rate=0.1):\n","        \"\"\"\n","        Initialise la couche de plongements de position, une couche dropout et les couches d'encodeurs\n","        Args :\n","            num_layers : Nombre de couches de décodeur\n","            dim_model : Dimension des embeddings du model\n","            num_heads : Nombre de têtes d'attention de l'encodeur\n","            feed_forward_size : Dimension du feed forward (en sortie)\n","            vocab_size : Taille du vocabulaire (correspondant à la taille d'entrée\n","            de la couche de plongements de position)\n","            dropout_rate : Ratio des entrées de la couche de dropout qui seront\n","            initialisés à zéro de manière aléatoire\n","        \"\"\"\n","        super().__init__()\n","\n","        self.dim_model = dim_model\n","        self.num_layers = num_layers\n","\n","        self.pos_embedding = PositionalEmbedding(input_size=vocab_size, dim_model=dim_model)\n","        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","        self.dec_layers = [DecoderLayer(dim_model=dim_model, num_heads=num_heads, feed_forward_size=feed_forward_size, dropout_rate=dropout_rate) for x in range(self.num_layers)]\n","\n","        self.last_attn_scores = None\n","\n","    def call(self, input, context, training):\n","        \"\"\"\n","        Execute la couche de plongements et de dropout\n","        puis toutes les couches de décodeurs\n","        \"\"\"\n","        input = self.dropout(self.pos_embedding(input))\n","        for i in range(self.num_layers):\n","            input = self.dec_layers[i](input, context, training)\n","\n","        return input"]},{"cell_type":"markdown","metadata":{"id":"nVZN8skQdkvp"},"source":["#### 4.6 Transformer (4 points)"]},{"cell_type":"markdown","metadata":{"id":"VKU-ezo1-8s7"},"source":["Le Transformer est maintenant prêt à être créé. Le constructeur s'occupe déjà d'initialiser tous les attributs nécessaires à son fonctionnement.\n","\n","La fonction `call` prend en entrées les arguments suivants :\n","- `inputs` : Les entrées du modèle de la forme d'un tuple regroupant l'entrée sparql et l'entrée anglaise (`inputs = (sparql, english)`)\n","- `training` : Valeur booléenne indiquant si le modèle est en entrainement ou pas\n","\n","La méthode `call` doit :\n","1. Séparer les entrées reçues en sparql et english\n","2. Envoyer les phrases sparql à l'encodeur\n","3. Envoyer les phrases en anglais au décodeur avec comme contexte la sortie de l'encodeur\n","4. Envoyer la sortie du décodeur à la couche dense initialisée dans le constructeur (`self.dense_layer`)\n","5. Appeler la fonction `drop_mask` avec comme argument les probabilités générées par la couche dense (en enlevant l'attribut `_keras_mask` des probabilités générées par la couche dense, on évite au modèle d'utiliser ce masque lorsqu'il calcule les métriques et le coût)"]},{"cell_type":"code","execution_count":152,"metadata":{"id":"NMNM6hDldKs6","executionInfo":{"status":"ok","timestamp":1698289732434,"user_tz":240,"elapsed":3,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","    \"\"\"\n","    Classe représentant le Transformer\n","    \"\"\"\n","\n","    def __init__(self, *, num_layers, dim_model, num_heads, feed_forward_size,\n","                input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","        \"\"\"\n","        Initialise les couches d'encodeur et de décodeurs et la couche dense finale\n","        Args :\n","            num_layers : Nombre de couches de décodeur\n","            dim_model : Dimension des embeddings du model\n","            num_heads : Nombre de têtes d'attention de l'encodeur et du décodeur\n","            feed_forward_size : Dimension du feed forward (en sortie)\n","            input_vocab_size : Taille du vocabulaire d'entrée\n","            target_vocab_Size : Taille du vocabulaire de sortie\n","            dropout_rate : Ratio des entrées de la couche de dropout qui seront\n","            initialisés à zéro de manière aléatoire\n","        \"\"\"\n","        super().__init__()\n","        self.encoder = Encoder(num_layers=num_layers, dim_model=dim_model,\n","                            num_heads=num_heads, feed_forward_size=feed_forward_size,\n","                            vocab_size=input_vocab_size,\n","                            dropout_rate=dropout_rate)\n","\n","        self.decoder = Decoder(num_layers=num_layers, dim_model=dim_model,\n","                            num_heads=num_heads, feed_forward_size=feed_forward_size,\n","                            vocab_size=target_vocab_size,\n","                            dropout_rate=dropout_rate)\n","\n","        self.dense_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inputs, training=True):\n","        \"\"\"\n","        Appel les couches d'encodeur et de décodeur avec les bonnes entrées et\n","        contexte ainsi que la couche dense finale\n","        \"\"\"\n","        sparql, english = inputs\n","        enc_output = self.encoder(sparql, training)\n","        dec_output = self.decoder(english, enc_output, training)\n","        final_output = self.dense_layer(dec_output)\n","        self.drop_mask(training, final_output)\n","        return final_output\n","\n","    def drop_mask(self, training, probabilities):\n","        if not training:\n","            try:\n","                del probabilities._keras_mask\n","            except AttributeError:\n","                pass"]},{"cell_type":"markdown","metadata":{"id":"Vn-QaVNs-8s7"},"source":["Vous pouvez tester votre implémentation finale du Transformer avec la fonction suivante. **Attention, ce n'est pas parce que vous obtenez les bons résultats qu'il n'y a pas de bugs dans votre implémentation, mais c'est déjà un bon signe**"]},{"cell_type":"code","execution_count":153,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":838,"status":"ok","timestamp":1698289733269,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"wLMtL7-l-8s7","outputId":"22ade95d-7d00-458a-8fdc-35542e2f5be6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[ 0.00026988 -0.03291521]\n","   [ 0.00026987 -0.03291528]]\n","\n","  [[ 0.00026988 -0.03291521]\n","   [ 0.00026988 -0.03291521]]]\n","\n","\n"," [[[ 0.00026985 -0.03291543]\n","   [ 0.00026987 -0.03291528]]\n","\n","  [[ 0.00026987 -0.03291528]\n","   [ 0.00026987 -0.03291528]]]], shape=(2, 2, 2, 2), dtype=float32)\n"]}],"source":["def test_transformer():\n","\n","    config = {\n","        'num_layers': 2,\n","        'dim_model': 2,\n","        'num_heads': 2,\n","        'feed_forward_size': 2,\n","        'input_vocab_size': 2,\n","        'target_vocab_size': 2,\n","        'dropout_rate': 0.1\n","    }\n","\n","    initializer = tf.keras.initializers.glorot_normal(42)\n","\n","    feed_forward = FeedForward(2, 2, 0.1)\n","    feed_forward.seq = tf.keras.Sequential([\n","        tf.keras.layers.Dense(2, activation='relu', kernel_initializer=initializer, use_bias=False),\n","        tf.keras.layers.Dense(2, kernel_initializer=initializer, use_bias=False),\n","        tf.keras.layers.Dropout(0.1, seed=42)\n","    ])\n","    feed_forward.add = tf.keras.layers.Add()\n","    feed_forward.layer_norm = tf.keras.layers.LayerNormalization(beta_initializer=initializer, gamma_initializer=initializer)\n","\n","    transformer = Transformer(**config)\n","\n","    transformer.encoder.pos_embedding.embedding_layer = tf.keras.layers.Embedding(2, 2, embeddings_initializer=initializer, mask_zero=False)\n","    for l in transformer.encoder.enc_layers:\n","        l.self_attention = GlobalSelfAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n","        l.ffn = feed_forward\n","    transformer.encoder.dropout = tf.keras.layers.Dropout(0.1, seed=42)\n","\n","    transformer.decoder.pos_embedding.embedding_layer = tf.keras.layers.Embedding(2, 2, embeddings_initializer=initializer, mask_zero=True)\n","    for l in transformer.decoder.dec_layers:\n","        l.cross_attention = CrossAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n","        l.encoder_decoder_attention = DecoderAttention(num_heads=2, key_dim=2, dropout=0.1, kernel_initializer=initializer)\n","        l.ffn = feed_forward\n","\n","    transformer.dense_layer = tf.keras.layers.Dense(2, kernel_initializer=initializer, use_bias=False)\n","    transformer.decoder.dropout = tf.keras.layers.Dropout(0.1, seed=42)\n","\n","    # Create determinisitc inputs and context\n","    generator = tf.random.Generator.from_seed(1)\n","    input = generator.normal(shape=(2, 2, 2))\n","    context = generator.normal(shape=(2, 2, 2))\n","\n","    input_transformer = (input, context)\n","    output = transformer(input_transformer, training=True)\n","    print(output)\n","\n","test_transformer()"]},{"cell_type":"markdown","metadata":{"id":"AB5h26oT-8s8"},"source":["```\n","tf.Tensor(\n","[[[[ 0.00026983 -0.03291529]\n","   [ 0.00026987 -0.03291498]]\n","\n","  [[ 0.00026987 -0.03291498]\n","   [ 0.00026987 -0.03291498]]]\n","\n","\n"," [[[ 0.00026983 -0.03291529]\n","   [ 0.00026987 -0.03291498]]\n","\n","  [[ 0.00026987 -0.03291498]\n","   [ 0.00026987 -0.03291498]]]], shape=(2, 2, 2, 2), dtype=float32)\n","```"]},{"cell_type":"markdown","metadata":{"id":"vCaWH9jCd51q"},"source":["#### 4.7 Scheduler"]},{"cell_type":"markdown","metadata":{"id":"JjLEk9ZB-8s8"},"source":["La classe `Scheduler` permet entre autre de mettre à jour le taux d'apprentissage du modèle lors de l'entraînement. Son implémentation complète vous est fournie."]},{"cell_type":"code","execution_count":154,"metadata":{"id":"R7KODnRNdeHW","executionInfo":{"status":"ok","timestamp":1698289733270,"user_tz":240,"elapsed":9,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Scheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, dim_model, warmup_steps):\n","        super().__init__()\n","        self.dim_model = tf.cast(dim_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)\n","        return tf.math.rsqrt(self.dim_model) * tf.math.minimum(tf.math.rsqrt(step), step * (self.warmup_steps ** -1.5))\n","\n","    def get_config(self):\n","        config = {\n","            'd_model': self.dim_model,\n","            'warmup_steps': self.warmup_steps,\n","        }\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"x-wLAeNReP_s"},"source":["### 5. Entrainement du modèle (15 points)"]},{"cell_type":"markdown","metadata":{"id":"LF78fuM5-8s8"},"source":["Il est maintenant temps de créer le traducteur qui va transformer des requêtes sparql en anglais. Pour cela, il faudra compléter 4 méthodes de la classe `Translator`, soit les méthodes `prepare`, `fit` et `translate`.\n","___\n","\n","La fonction `prepare` reçoit les données d'entrainement et de validation sous forme de pandas DataFrame et s'occupe de :\n","1. Appeler le préprocesseur sur les données d'entrainement et de validation\n","2. Créer un objet `tf.Dataset` contenant un tuple des requêtes sparql et des questions en anglais pour l'ensemble d'entraînement et de validation\n","3. Envoyer les 2 datasets créés (entraînement et validation) au batcher\n","\n","Elle retourne un tuple contenant les batches d'entraînement et de validation\n","\n","___\n","\n","La fonction `fit` s'occupe simplement d'entraîner le modèle avec les données d'entraînement et de validation passés en paramètre.\n","___\n","\n","La fonction `translate` s'occupe de traduire une série de données sparql en anglais. Pour cela, plusieurs étapes doivent être effectuées. Elle doit :\n","1. Appliquer le préprocesseur sur l'ensemble de test donné\n","2. Créer des batches à l'aide du batcher de test\n","3. Pour chaque valeur dans les batches créés\n","  - Extraire le contenu du tuple. Souvenez-vous que ce qui est ressorti par la méthode `prepare_batch` dans le cas d'un batcher de test est une tuple de la forme (phrase SPARQL, phrase anglais) où initialement, la phrase anglaise est initialisée avec le jeton de départ\n","  - Envoyer les contextes et les phrases au Transformer pour qu'il prédise le prochain jeton\n","  - Concaténer ensemble tous les jetons prédits par le Transformer pour générer la traduction  \n","4. Réduire la taille des prédictions pour enlever tout ce qui vient après le jeton de fin généré par le Transformer (si aucun jeton de fin n'est généré, la traduction n'a pas besoin d'être coupée)\n","5. Transformer les jetons prédits en mots à l'aide du bon tokenizer\n","6. Annuler les transformations initiales effectuées à l'aide du pré-traitement\n","\n","\n","Les fonctions `masked_loss` et `masked_accuracy` vous sont fournies et permettent d'évaluer l'exactitude du Transformer en évaluant une fonction de perte propre au Transformer."]},{"cell_type":"code","execution_count":155,"metadata":{"id":"i_Ta5Z0YfnA1","executionInfo":{"status":"ok","timestamp":1698289733270,"user_tz":240,"elapsed":8,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["class Translator:\n","\n","    num_layers = 4\n","    dim_model = 128\n","    feed_forward_size = 512\n","    num_heads = 6\n","    dropout_rate = 0.1\n","    input_vocab_size = 8000\n","    target_vocab_size = 8000\n","    batch_size = 64\n","    batch_size_test = 500\n","    buffer_size = 20000\n","    buffer_size_test = None\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialise le preprocessor, les tokenizers, les batchers et le Transformer\n","        avec les bons paramètres\n","        \"\"\"\n","\n","        self.pre_processor = Preprocessor()\n","\n","        self.tokenizers = GroupedTokenizers(\n","            LanguageTokenizer.reserved_tokens,\n","            root + 'language_vocab_english.txt',\n","            root + 'language_vocab_sparql.txt'\n","        )\n","\n","        self.train_batcher = Batcher(tokenizers=self.tokenizers, train=True, max_tokens=Translator.dim_model, batch_size=Translator.batch_size, buffer_size=Translator.buffer_size)\n","        self.test_batcher = Batcher(tokenizers=self.tokenizers, train=False, max_tokens=Translator.dim_model, batch_size=Translator.batch_size_test, buffer_size=Translator.buffer_size_test)\n","\n","        self.transformer = Transformer(\n","            num_layers=Translator.num_layers,\n","            dim_model=Translator.dim_model,\n","            num_heads=Translator.num_heads,\n","            feed_forward_size=Translator.feed_forward_size,\n","            input_vocab_size=Translator.input_vocab_size,\n","            target_vocab_size=Translator.target_vocab_size,\n","            dropout_rate=Translator.dropout_rate)\n","\n","        self.scheduler = Scheduler(Translator.dim_model, 4000)\n","        self.optimizer = tf.keras.optimizers.Adam(self.scheduler, beta_1=0.9, beta_2=0.95, epsilon=1e-9)\n","\n","        self.transformer.compile(\n","            loss=Translator.masked_loss,\n","            optimizer=self.optimizer,\n","            metrics=[Translator.masked_accuracy])\n","\n","        self.end = self.tokenizers.sparql.tokenize([''])[0][1][tf.newaxis]\n","\n","    def prepare(self, train: pd.DataFrame, val: pd.DataFrame):\n","        \"\"\"\n","        Prépare les ensembles de validation et d'entrainement à l'entrainement\n","        en les envoyant au preprocessor et au batcher\n","        Args :\n","            - train : DataFrame d'entrainement avec les columns sparql (entrée) et anglais (sortie)\n","            - val : DataFrame de validation avec les columns sparql (entrée) et anglais (sortie)\n","\n","        Returns :\n","            Tuple contenant les batches d'entraînement et les batches de validation\n","        \"\"\"\n","        # Preprocessing des données\n","        train = self.pre_processor.transform_dataframe(train)\n","        val = self.pre_processor.transform_dataframe(val)\n","\n","        # Création des datasets tensorflow et zippage\n","        train_english = tf.data.Dataset.from_tensor_slices(train['english'])\n","        train_sparql = tf.data.Dataset.from_tensor_slices(train['sparql'])\n","        train = tf.data.Dataset.zip((train_sparql, train_english))\n","\n","        val_english = tf.data.Dataset.from_tensor_slices(val['english'])\n","        val_sparql = tf.data.Dataset.from_tensor_slices(val['sparql'])\n","        val = tf.data.Dataset.zip((val_sparql, val_english))\n","\n","        # Création des batches\n","        train = self.train_batcher.make_batches(train)\n","        val = self.train_batcher.make_batches(val)\n","\n","        return train, val\n","\n","    def fit(self, training, validation, epochs=50):\n","        \"\"\"\n","        Entraine le modèle en utilisant l'ensemble d'entrainement et valide le résultat\n","        \"\"\"\n","\n","        self.transformer.fit(training, validation_data=validation, epochs=epochs)\n","\n","\n","    def translate(self, sparql: pd.Series):\n","        \"\"\"\n","        Traduit une série de requêtes sparql en anglais\n","        \"\"\"\n","\n","        # Preprocessing des données\n","        sparql = self.pre_processor.transform_sparql(sparql)\n","\n","        # Création du dataset tensorflow\n","        sparql = tf.data.Dataset.from_tensor_slices(sparql)\n","\n","        # On crée les batches\n","        sparql = self.test_batcher.make_batches(sparql)\n","\n","        translation = pd.Series()\n","\n","        for batch in sparql:\n","\n","          input, context = batch\n","\n","          for i in range(self.dim_model):\n","            pred = self.transformer((input, context) , False)\n","\n","            predictions = pred[:,-1,:]\n","            predicted_id = [[x] for x in tf.argmax(predictions, axis=-1)]\n","\n","            context = tf.concat([context, predicted_id], axis=-1)\n","\n","          # On transforme les jetons prédits en mots\n","          english = self.tokenizers.english.detokenize(context)\n","          english = pd.Series(english)\n","          english = english.map(self.pre_processor.transform_back_english)\n","\n","          # On rassemble avec les autres batchs\n","          translation = pd.concat([translation, english], ignore_index=True)\n","\n","        return english\n","\n","    def masked_loss(label, pred):\n","        mask = label != 0\n","        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=True, reduction='none')\n","        loss = loss_object(label, pred)\n","\n","        mask = tf.cast(mask, dtype=loss.dtype)\n","        loss *= mask\n","\n","        loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","        return loss\n","\n","    def masked_accuracy(label, pred):\n","        pred = tf.argmax(pred, axis=2)\n","        label = tf.cast(label, pred.dtype)\n","        match = label == pred\n","\n","        mask = label != 0\n","\n","        match = match & mask\n","\n","        match = tf.cast(match, dtype=tf.float32)\n","        mask = tf.cast(mask, dtype=tf.float32)\n","        return tf.reduce_sum(match)/tf.reduce_sum(mask)"]},{"cell_type":"markdown","metadata":{"id":"aqrPATrw-8s9"},"source":["#### 5.1 Préparation des données\n","\n","Exécuter ensuite la cellule ci-dessous pour créer maintenant une instance de la classe `Translator`, charger les données d'entraînement et de validation et préparer les données à l'entraînement"]},{"cell_type":"code","execution_count":156,"metadata":{"executionInfo":{"elapsed":3889,"status":"ok","timestamp":1698289737151,"user":{"displayName":"Lucas","userId":"03809761009706358513"},"user_tz":240},"id":"bPVlAR77eRlt"},"outputs":[],"source":["translator = Translator()\n","\n","data_loader = DataLoader(\n","    training_path=root + 'train.csv',\n","    validation_path=root + 'validation.csv'\n",")\n","\n","train_batch, val_batch = translator.prepare(data_loader.train, data_loader.val)"]},{"cell_type":"markdown","metadata":{"id":"t6DR4ZX5-8s9"},"source":["#### 5.2 Entraînement\n","\n","Entraînez le modèle avec les données"]},{"cell_type":"code","execution_count":157,"metadata":{"id":"1u45C28kr0vS","executionInfo":{"status":"ok","timestamp":1698290652374,"user_tz":240,"elapsed":915236,"user":{"displayName":"Lucas","userId":"03809761009706358513"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df7b1b8f-f861-4c8c-a1c7-336c9d4c41a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","63/63 [==============================] - 59s 358ms/step - loss: 8.8042 - masked_accuracy: 0.0977 - val_loss: 8.5662 - val_masked_accuracy: 0.1747\n","Epoch 2/50\n","63/63 [==============================] - 14s 217ms/step - loss: 8.3740 - masked_accuracy: 0.1871 - val_loss: 8.0918 - val_masked_accuracy: 0.2201\n","Epoch 3/50\n","63/63 [==============================] - 13s 202ms/step - loss: 7.7691 - masked_accuracy: 0.2410 - val_loss: 7.3559 - val_masked_accuracy: 0.2578\n","Epoch 4/50\n","63/63 [==============================] - 13s 208ms/step - loss: 6.9453 - masked_accuracy: 0.2616 - val_loss: 6.4652 - val_masked_accuracy: 0.2671\n","Epoch 5/50\n","63/63 [==============================] - 13s 211ms/step - loss: 6.0288 - masked_accuracy: 0.2684 - val_loss: 5.5558 - val_masked_accuracy: 0.2738\n","Epoch 6/50\n","63/63 [==============================] - 13s 212ms/step - loss: 5.1750 - masked_accuracy: 0.2859 - val_loss: 4.7798 - val_masked_accuracy: 0.2976\n","Epoch 7/50\n","63/63 [==============================] - 13s 210ms/step - loss: 4.4656 - masked_accuracy: 0.3475 - val_loss: 4.1282 - val_masked_accuracy: 0.4005\n","Epoch 8/50\n","63/63 [==============================] - 13s 210ms/step - loss: 3.8311 - masked_accuracy: 0.4185 - val_loss: 3.5260 - val_masked_accuracy: 0.4276\n","Epoch 9/50\n","63/63 [==============================] - 12s 191ms/step - loss: 3.3060 - masked_accuracy: 0.4493 - val_loss: 3.1111 - val_masked_accuracy: 0.4556\n","Epoch 10/50\n","63/63 [==============================] - 12s 189ms/step - loss: 2.9889 - masked_accuracy: 0.4714 - val_loss: 2.8891 - val_masked_accuracy: 0.4697\n","Epoch 11/50\n","63/63 [==============================] - 13s 197ms/step - loss: 2.8095 - masked_accuracy: 0.4891 - val_loss: 2.7518 - val_masked_accuracy: 0.4867\n","Epoch 12/50\n","63/63 [==============================] - 12s 193ms/step - loss: 2.6969 - masked_accuracy: 0.4974 - val_loss: 2.6611 - val_masked_accuracy: 0.4921\n","Epoch 13/50\n","63/63 [==============================] - 13s 204ms/step - loss: 2.6064 - masked_accuracy: 0.5049 - val_loss: 2.5788 - val_masked_accuracy: 0.5023\n","Epoch 14/50\n","63/63 [==============================] - 12s 188ms/step - loss: 2.5194 - masked_accuracy: 0.5161 - val_loss: 2.4920 - val_masked_accuracy: 0.5196\n","Epoch 15/50\n","63/63 [==============================] - 12s 196ms/step - loss: 2.4329 - masked_accuracy: 0.5303 - val_loss: 2.4267 - val_masked_accuracy: 0.5240\n","Epoch 16/50\n","63/63 [==============================] - 13s 200ms/step - loss: 2.3487 - masked_accuracy: 0.5432 - val_loss: 2.3306 - val_masked_accuracy: 0.5432\n","Epoch 17/50\n","63/63 [==============================] - 12s 188ms/step - loss: 2.2585 - masked_accuracy: 0.5557 - val_loss: 2.2385 - val_masked_accuracy: 0.5613\n","Epoch 18/50\n","63/63 [==============================] - 13s 207ms/step - loss: 2.1462 - masked_accuracy: 0.5764 - val_loss: 2.1086 - val_masked_accuracy: 0.5868\n","Epoch 19/50\n","63/63 [==============================] - 12s 189ms/step - loss: 2.0051 - masked_accuracy: 0.6046 - val_loss: 1.9281 - val_masked_accuracy: 0.6222\n","Epoch 20/50\n","63/63 [==============================] - 12s 194ms/step - loss: 1.8451 - masked_accuracy: 0.6295 - val_loss: 1.7562 - val_masked_accuracy: 0.6425\n","Epoch 21/50\n","63/63 [==============================] - 13s 205ms/step - loss: 1.6804 - masked_accuracy: 0.6555 - val_loss: 1.6474 - val_masked_accuracy: 0.6644\n","Epoch 22/50\n","63/63 [==============================] - 12s 193ms/step - loss: 1.5213 - masked_accuracy: 0.6804 - val_loss: 1.4173 - val_masked_accuracy: 0.7060\n","Epoch 23/50\n","63/63 [==============================] - 12s 196ms/step - loss: 1.3524 - masked_accuracy: 0.7113 - val_loss: 1.2445 - val_masked_accuracy: 0.7392\n","Epoch 24/50\n","63/63 [==============================] - 12s 196ms/step - loss: 1.1872 - masked_accuracy: 0.7452 - val_loss: 1.0550 - val_masked_accuracy: 0.7756\n","Epoch 25/50\n","63/63 [==============================] - 13s 201ms/step - loss: 1.0151 - masked_accuracy: 0.7814 - val_loss: 0.8968 - val_masked_accuracy: 0.8075\n","Epoch 26/50\n","63/63 [==============================] - 12s 186ms/step - loss: 0.8808 - masked_accuracy: 0.8089 - val_loss: 0.7800 - val_masked_accuracy: 0.8316\n","Epoch 27/50\n","63/63 [==============================] - 12s 191ms/step - loss: 0.7465 - masked_accuracy: 0.8356 - val_loss: 0.7018 - val_masked_accuracy: 0.8454\n","Epoch 28/50\n","63/63 [==============================] - 13s 202ms/step - loss: 0.6557 - masked_accuracy: 0.8518 - val_loss: 0.6292 - val_masked_accuracy: 0.8595\n","Epoch 29/50\n","63/63 [==============================] - 13s 202ms/step - loss: 0.5988 - masked_accuracy: 0.8618 - val_loss: 0.5796 - val_masked_accuracy: 0.8660\n","Epoch 30/50\n","63/63 [==============================] - 12s 188ms/step - loss: 0.5104 - masked_accuracy: 0.8785 - val_loss: 0.5112 - val_masked_accuracy: 0.8815\n","Epoch 31/50\n","63/63 [==============================] - 12s 195ms/step - loss: 0.4799 - masked_accuracy: 0.8830 - val_loss: 0.5535 - val_masked_accuracy: 0.8675\n","Epoch 32/50\n","63/63 [==============================] - 13s 201ms/step - loss: 0.4314 - masked_accuracy: 0.8917 - val_loss: 0.4685 - val_masked_accuracy: 0.8909\n","Epoch 33/50\n","63/63 [==============================] - 12s 186ms/step - loss: 0.3981 - masked_accuracy: 0.8982 - val_loss: 0.4155 - val_masked_accuracy: 0.9003\n","Epoch 34/50\n","63/63 [==============================] - 12s 190ms/step - loss: 0.3555 - masked_accuracy: 0.9081 - val_loss: 0.4280 - val_masked_accuracy: 0.8970\n","Epoch 35/50\n","63/63 [==============================] - 13s 203ms/step - loss: 0.3409 - masked_accuracy: 0.9107 - val_loss: 0.3842 - val_masked_accuracy: 0.9065\n","Epoch 36/50\n","63/63 [==============================] - 12s 186ms/step - loss: 0.3176 - masked_accuracy: 0.9158 - val_loss: 0.4573 - val_masked_accuracy: 0.8937\n","Epoch 37/50\n","63/63 [==============================] - 12s 195ms/step - loss: 0.2955 - masked_accuracy: 0.9205 - val_loss: 0.3546 - val_masked_accuracy: 0.9154\n","Epoch 38/50\n","63/63 [==============================] - 14s 213ms/step - loss: 0.2738 - masked_accuracy: 0.9256 - val_loss: 0.3401 - val_masked_accuracy: 0.9181\n","Epoch 39/50\n","63/63 [==============================] - 14s 221ms/step - loss: 0.2710 - masked_accuracy: 0.9257 - val_loss: 0.3555 - val_masked_accuracy: 0.9168\n","Epoch 40/50\n","63/63 [==============================] - 13s 208ms/step - loss: 0.2405 - masked_accuracy: 0.9337 - val_loss: 0.3685 - val_masked_accuracy: 0.9141\n","Epoch 41/50\n","63/63 [==============================] - 12s 192ms/step - loss: 0.2419 - masked_accuracy: 0.9329 - val_loss: 0.3141 - val_masked_accuracy: 0.9268\n","Epoch 42/50\n","63/63 [==============================] - 13s 207ms/step - loss: 0.2239 - masked_accuracy: 0.9377 - val_loss: 0.3636 - val_masked_accuracy: 0.9166\n","Epoch 43/50\n","63/63 [==============================] - 16s 259ms/step - loss: 0.2174 - masked_accuracy: 0.9393 - val_loss: 0.3232 - val_masked_accuracy: 0.9235\n","Epoch 44/50\n","63/63 [==============================] - 13s 215ms/step - loss: 0.2089 - masked_accuracy: 0.9413 - val_loss: 0.3200 - val_masked_accuracy: 0.9282\n","Epoch 45/50\n","63/63 [==============================] - 12s 187ms/step - loss: 0.1968 - masked_accuracy: 0.9443 - val_loss: 0.2846 - val_masked_accuracy: 0.9348\n","Epoch 46/50\n","63/63 [==============================] - 15s 244ms/step - loss: 0.1867 - masked_accuracy: 0.9462 - val_loss: 0.3120 - val_masked_accuracy: 0.9297\n","Epoch 47/50\n","63/63 [==============================] - 13s 203ms/step - loss: 0.1765 - masked_accuracy: 0.9489 - val_loss: 0.3325 - val_masked_accuracy: 0.9283\n","Epoch 48/50\n","63/63 [==============================] - 14s 226ms/step - loss: 0.1669 - masked_accuracy: 0.9522 - val_loss: 0.3180 - val_masked_accuracy: 0.9299\n","Epoch 49/50\n","63/63 [==============================] - 13s 204ms/step - loss: 0.1774 - masked_accuracy: 0.9500 - val_loss: 0.3065 - val_masked_accuracy: 0.9330\n","Epoch 50/50\n","63/63 [==============================] - 13s 205ms/step - loss: 0.1618 - masked_accuracy: 0.9539 - val_loss: 0.2961 - val_masked_accuracy: 0.9338\n"]}],"source":["translator.fit(train_batch, val_batch, epochs=50)"]},{"cell_type":"markdown","metadata":{"id":"5bFPpa4D-8s9"},"source":["#### 5.3 Traduction\n","\n","Effectuez la traduction des données de test pour valider l'efficacité du modèle"]},{"cell_type":"code","execution_count":158,"metadata":{"id":"aLcN1y9GLqwH","colab":{"base_uri":"https://localhost:8080/","height":1347},"executionInfo":{"status":"ok","timestamp":1698291361252,"user_tz":240,"elapsed":708887,"user":{"displayName":"Lucas","userId":"03809761009706358513"}},"outputId":"678f39f2-1a0c-4806-80fe-3d7483c9aff9"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-155-dcf0ecb89d5e>:103: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  translation = pd.Series()\n"]},{"output_type":"execute_result","data":{"text/plain":["                                           prediction  \\\n","0   give me a count of bacterias whose dbo:order i...   \n","1   what is the dbo:televisionshow whose dbo:netwo...   \n","2   what is the dbo:routestart of dbr:moscow - kaz...   \n","3   is dbr:postgresql the dbp:programminglanguage ...   \n","4   what is the settlement whose dbp:neighboringmu...   \n","..                                                ...   \n","95  what is the dbp:residence of the president who...   \n","96  what is the dbo:scientist whose dbo:doctoralad...   \n","97  what is the dbo:scientist whose dbp:doctoralst...   \n","98  what are the dbo:film whose dbp:writer is dbr:...   \n","99  what is the dbp:developer of dbr:lao_plaza_hot...   \n","\n","                                          target_text  \n","0   what is the dbo_religion of the dbo_PoliticalP...  \n","1   what is the dbo_knownFor of the dbr_Sam_Loyd a...  \n","2   who is the dbo_associatedBand of the dbr_Joe_P...  \n","3   what is the dbp_nationalOrigin of the dbr_Fock...  \n","4   what is the dbp_artist of the dbr_Women_in_the...  \n","..                                                ...  \n","95  whose dbp_relatives are dbr_Clan_McDuck and db...  \n","96  who is the dbo_firstAscentPerson of the dbr_Ca...  \n","97  how many other dbo_homeStadium are there of th...  \n","98  who is the dbp_composer of the dbr_Motorpsycho...  \n","99  what is the dbo_religion of the president who ...  \n","\n","[100 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-750ae82b-582f-48f7-a894-b51f410c86eb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prediction</th>\n","      <th>target_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>give me a count of bacterias whose dbo:order i...</td>\n","      <td>what is the dbo_religion of the dbo_PoliticalP...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>what is the dbo:televisionshow whose dbo:netwo...</td>\n","      <td>what is the dbo_knownFor of the dbr_Sam_Loyd a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what is the dbo:routestart of dbr:moscow - kaz...</td>\n","      <td>who is the dbo_associatedBand of the dbr_Joe_P...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>is dbr:postgresql the dbp:programminglanguage ...</td>\n","      <td>what is the dbp_nationalOrigin of the dbr_Fock...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>what is the settlement whose dbp:neighboringmu...</td>\n","      <td>what is the dbp_artist of the dbr_Women_in_the...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>what is the dbp:residence of the president who...</td>\n","      <td>whose dbp_relatives are dbr_Clan_McDuck and db...</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>what is the dbo:scientist whose dbo:doctoralad...</td>\n","      <td>who is the dbo_firstAscentPerson of the dbr_Ca...</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>what is the dbo:scientist whose dbp:doctoralst...</td>\n","      <td>how many other dbo_homeStadium are there of th...</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>what are the dbo:film whose dbp:writer is dbr:...</td>\n","      <td>who is the dbp_composer of the dbr_Motorpsycho...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>what is the dbp:developer of dbr:lao_plaza_hot...</td>\n","      <td>what is the dbo_religion of the president who ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-750ae82b-582f-48f7-a894-b51f410c86eb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-750ae82b-582f-48f7-a894-b51f410c86eb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-750ae82b-582f-48f7-a894-b51f410c86eb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-010a8cba-877e-463c-9300-88467688eb06\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-010a8cba-877e-463c-9300-88467688eb06')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-010a8cba-877e-463c-9300-88467688eb06 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":158}],"source":["predictions = translator.translate(data_loader.train['sparql'])\n","formatted_predictions = pd.concat([pd.DataFrame(predictions), data_loader.val], axis=1)\n","formatted_predictions.drop(['id', 'sparql'], inplace=True, axis=1)\n","formatted_predictions.rename(columns={0:'prediction', 'english':'target_text'}, inplace=True)\n","formatted_predictions.head(100)"]},{"cell_type":"markdown","metadata":{"id":"UqZbQNBd-ikk"},"source":[]},{"cell_type":"markdown","metadata":{"id":"7ZSxn1aTholW"},"source":["### 6. Évalution : Métrique BLEU (15 points)\n","\n","Pour évaluer l'efficacité des traductions, la métrique BLEU sera utilisée. La formule est donnée ci-dessous :\n","$$BLEU = BP * exp \\Big( \\sum_{n=1}^{N} w_n log p_n \\Big)$$\n","\n","où $p_n$ est la précision modifiée pour le n-gramme (correspondant au ratio de la fréquence maximum du n-gramme dans chaque phrase de référence par la fréquence du n-gramme). Posons ensuite $r$ comme le nombre de mots dans la phrase cible et $c$ comme le nombre de mots dans la phrase prédite. Si $c>r$, alors BP vaut 1. Sinon $BP = exp(1 - \\frac{r}{c})$.\n","\n","Les valeurs des poids $w_n$ est ce qui donne les différentes variations de la métrique BLEU. Dans notre cas, la métrique BLEU-3 sera utilisée."]},{"cell_type":"code","execution_count":162,"metadata":{"id":"EoCj2WfhiMKW","executionInfo":{"status":"ok","timestamp":1698291441785,"user_tz":240,"elapsed":205,"user":{"displayName":"Lucas","userId":"03809761009706358513"}}},"outputs":[],"source":["def evaluate_model(data: pd.DataFrame):\n","    \"\"\"\n","    Évalue la précision du modèle en utilisant la métrique BLEU\n","    Args :\n","        - data : DataFrame contenant deux colonnes (predictions et target_text)\n","\n","    Returns :\n","        La moyenne du score BLEU\n","    \"\"\"\n","    weights = (1/3, 1/3, 1/3) # Use Bleu-3\n","    scores = np.zeros(data.shape[0])\n","    index = 0\n","    for iter, row in data.iterrows():\n","        if not pd.notnull(row['prediction']):\n","            continue\n","        prediction = row['prediction'].split()\n","        target_text = row['target_text'].split()\n","\n","        scores[index] = sentence_bleu([target_text], prediction, weights=weights)\n","\n","        index += 1\n","    return np.mean(scores)\n"]},{"cell_type":"markdown","metadata":{"id":"OyoCX4K5-8s-"},"source":["#### 6.1 Évaluation du modèle\n","\n","Appelez la fonction `evaluate_model` sur les prédictions de votre modèle pour évaluer sa performance."]},{"cell_type":"code","execution_count":163,"metadata":{"id":"aZVLOVd2wSfd","executionInfo":{"status":"ok","timestamp":1698291443815,"user_tz":240,"elapsed":211,"user":{"displayName":"Lucas","userId":"03809761009706358513"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8e50ec6-77b3-4403-a469-113c9b345f54"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.06297190896547197"]},"metadata":{},"execution_count":163}],"source":["evaluate_model(formatted_predictions)"]},{"cell_type":"markdown","metadata":{"id":"0POSjEaM-8s-"},"source":["#### 6.2. Analyse des erreurs (10 points)\n","Analysez les traductions du modèle et ses erreurs. Implantez une analyse statistique  (selon la forme de votre choix) qui affiche des catégories d'erreurs et leur % d'occurrence parmi l'ensemble des erreurs possibles. Vous pouvez orienter votre fonction pour qu'elle décrive des dimensions spécifiques. Par exemple : les erreurs sont-elles plus souvent sur les éléments de la base de connaissance \"dbx\" ou sur le reste des jetons ? Les erreurs sont-elles dues à des éléments qui ne sont pas vus en entrainement ?\n"]},{"cell_type":"markdown","metadata":{"id":"l-WENm2E-8s-"},"source":["> La métrique BLEU obtenue est fausse car nous n'avons pas rétiré les tokens après le token de fin.  \n","Par manque de temps nous n'avons pas pu implémenter l'analyse, voici cependant ce que nous souhaitions faire :  \n","- Comparer les longueurs de phrases entre celles prédites et celles attendues. Cela permettrait de voir si notre modèle à tendance à sur ou sous générer des phrases.\n","- Calculer le pourcentage d'apparition de chaque mot des phrases prédites. Ce pourcentage serait défini par le rapport entre nombre de fois qu'il apparait et le nombre de fois qu'il était attendu. Implémentation d'une manière similaire à bag of words des TP précédents. Cela permettrait de connaitre les mots qui ont plus ou moins tendance à apparaitre qu'à la normale. En utilisant leur contexte on pourrait également essayer de comprendre pourquoi leur fréquence d'apparition est différente.\n","- Faire une analyse de sens/sentiments pour estimer la ressemblance entre une phrase prédite et attendue ; estimer à quel point elle sont proches au niveau du sens.\n"]},{"cell_type":"markdown","metadata":{"id":"BIiIIxFb-8s-"},"source":["#### 6.3 Amélioration (5 points)\n","Donnez des pistes de solution pour améliorer le score BLEU"]},{"cell_type":"markdown","metadata":{"id":"-A5O6H0V-8s-"},"source":["> - Utilisation d'un modèle pré-entrainé qu'on adapterait à notre situation de traduction sparql -> english\n","- Base de données d'apprentissage plus grande\n","- Prendre en considération le contexte à droite également"]},{"cell_type":"markdown","metadata":{"id":"R3epadsh-8s_"},"source":["## LIVRABLES:\n","Vous devez remettre sur Moodle un zip contenant les fichiers suivants :\n","\n","1-\tLe code : Vous devez compléter le squelette inf8460_tp3.ipynb sous le nom   equipe_i_inf8460_TP3.ipynb (i = votre numéro d’équipe). Indiquez vos noms et matricules au début du notebook. Ce notebook doit contenir les fonctionnalités requises avec des commentaires appropriés. Le code doit être exécutable sans erreur et accompagné de commentaires appropriés de manière à expliquer les différentes fonctions. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ;\n","\n","2-\tUn fichier pdf représentant votre notebook complètement exécuté sous format pdf.\n","Pour créer le fichier cliquez sur File > Download as > PDF via LaTeX (.pdf). Assurez-vous que le PDF est entièrement lisible.\n","\n","\n","## EVALUATION\n","Votre TP sera évalué selon les critères suivants :\n","\n","1. Exécution correcte du code\n","2. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n","3. Commentaires clairs et informatifs\n","4. Performance attendue des modèles\n","5. Réponses correctes/sensées aux questions de réflexion ou d'analyse\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}