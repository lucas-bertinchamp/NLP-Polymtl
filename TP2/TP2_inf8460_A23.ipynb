{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-al-PAKXuHL"
      },
      "source": [
        "# INF8460: Traitement automatique de la langue naturelle\n",
        "\n",
        "### TP2: Autocomplétion et génération de phrases avec des modèles de langue n-grammes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKgBFhLxXuHO"
      },
      "source": [
        "## Identification de l'équipe:\n",
        "\n",
        "### Groupe de laboratoire:\n",
        "\n",
        "### Equipe numéro :\n",
        "\n",
        "### Membres:\n",
        "\n",
        "- Lucas Bertinchamp (2312324) (% de contribution, nature de la contribution)\n",
        "- Antoine Toussaint (2312379) (% de contribution, nature de la contribution)\n",
        "- Sebastian Villanueva (2087346) (% de contribution, nature de la contribution)\n",
        "\n",
        "* nature de la contribution: Décrivez brièvement ce qui a été fait par chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-JeQmwXuHP"
      },
      "source": [
        "TODOOOOOO : Q 2.4.2, valeurs différentes de ce qui est attendu. Random seed des jeux de train et test ?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x-_Y2OWXuHP"
      },
      "source": [
        "## Description:\n",
        "\n",
        "Ce deuxième travail pratique sera dédié à l'utilisation des n-grammes en traitement du langage naturel (NLP). Les n-grammes sont des séquences de n mots consécutifs dans un texte, et ils sont présent dans de nombreuses applications du NLP.\n",
        "\n",
        "Au cours de ce laboratoire, nous allons explorer comment les n-grammes sont utilisés pour des tâches telles que la prédiction de mots, l'autocomplétion et la génération de texte. Voici quelques exemples concrets de leur utilisation :\n",
        "\n",
        "Prédiction de Mots : Les n-grammes sont utilisés pour prédire le mot suivant étant donné le contexte précédent. Par exemple, dans la phrase \"Je pense donc je ____\", un modèle de bigramme pourrait suggérer \"suis\" comme mot suivant, basé sur des observations antérieures.\n",
        "\n",
        "Autocomplétion : Les moteurs de recherche utilisent les n-grammes pour proposer des suggestions de recherche lorsque vous commencez à taper une phrase. Ils se basent sur les préfixes du mot courant et sur les phrases écrites par l'utilisateur.\n",
        "\n",
        "Génération de Texte : Les n-grammes peuvent être utilisés pour générer automatiquement du texte. Ils aident un modèle à prédire les mots suivants en fonction des probabilités des mots pouvant être générés selon le contexte.\n",
        "\n",
        "Vous étudierez aussi une manière d'évaluer la qualité des modèles de langage génératifs en utilisant la perplexité. La perplexité est une métrique qui évalue la confiance avec laquelle un modèle est capable de prédire une séquence de mots dans un texte. Plus la perplexité est faible, plus le modèle est capable de prédire avec précision. Autrement dit, la perplexité est une mesure de la \"surprise\" d'un modèle lorsqu'il est exposé à un nouvel ensemble de mots. Nous utiliserons cette métrique pour évaluer nos modèles basés sur les n-grammes.\n",
        "\n",
        "\n",
        "**NOTE: seulement les librairies standards de python (et numpy) sont permises ainsi que celles déjà importées le notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buR2p9SoXuHQ"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1.  Chargement et pré-traitement des données (12 points)\n",
        "\n",
        "<a name='1.1'></a>\n",
        "### 1.1 Chargement des données (1 point)\n",
        "\n",
        "Les données que vous allez utiliser dans ce travail sont contenues dans le fichier [trump.txt](./trump.txt)\n",
        "\n",
        "Lisez le contenu de ce fichier et stockez-le dans une variable data.\n",
        "\n",
        "Affichez ensuite les 300 premiers caractères\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "K5gDTrHTXuHQ",
        "outputId": "6feaa13c-fee8-4623-85c3-ec8fdf560205"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Thank you very much.\\nWe had an amazing convention.\\nThat was one of the best.\\nI think it was one of the best ever.\\nIn terms -- in terms of enthusiasm, in terms of I think what it represents, getting our word out.\\nIvanka was incredible last night.\\nShe did an incredible job.\\nAnd so many of the speakers'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filename = \"trump.txt\"\n",
        "file = open(filename, 'r')\n",
        "data = file.read()\n",
        "\n",
        "data[:300]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz3rqIYXuHR"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2  Segmentation (2 points)\n",
        "\n",
        "Pré-traitez les données en suivant les étapes suivantes:\n",
        "\n",
        "1. Enlever les majuscules.\n",
        "2. Remplacer les \"\\n\" par des espaces\n",
        "3. Séparer les données en phrases en utilisant les délimiteurs suivants `.`, `?` et `!` comme séparateur.\n",
        "4. Enlever les signes de ponctuation (Attention de garder les espaces).\n",
        "5. Enlever les phrases vides.\n",
        "6. Segmenter les phrases avec la fonction nltk.word_tokenize()\n",
        "\n",
        "Utilisez ensuite votre fonction pour prétraiter le jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ssM-pmmmXuHR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def preprocess(data):\n",
        "    # Elever les majuscules\n",
        "    data_processed = data.lower()\n",
        "\n",
        "    # Remplacer les \"\\n\" par des espaces\n",
        "    data_processed = data_processed.replace('\\n', ' ')\n",
        "\n",
        "    # Séparer les données en phrases en utilisant les délimiteurs suivants `.`, `?` et `!` comme séparateur.\n",
        "    data_processed = re.split(r'[.?!]', data_processed)\n",
        "\n",
        "    # Enlever les signes de ponctuation (Attention de garder les espaces).\n",
        "    data_processed = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in data_processed]\n",
        "\n",
        "    # Enlever les phrases vides.\n",
        "    data_processed = [sentence for sentence in data_processed if sentence != '']\n",
        "\n",
        "    # Segmenter les phrases avec la fonction nltk.word_tokenize()\n",
        "    data_processed = [nltk.word_tokenize(sentence) for sentence in data_processed]\n",
        "\n",
        "    return data_processed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HaOHMTczXuHS"
      },
      "outputs": [],
      "source": [
        "data = preprocess(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dnBwQFinXuHS",
        "outputId": "8bb27f51-76fe-452e-cbd3-935aa28464c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "x = \"Cats are independent.\\nDogs are faithful.\"\n",
        "preprocess(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK7FMeh6XuHS"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_OwLapNXuHS"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "###  1.3 Création d'ensembles d'entraînement et de test (1 point)\n",
        "\n",
        "Échantilloner de manière **aléatoire** 80% des données pour l'ensemble d'entrainement. Garder 20% pour l'ensemble de test. Utilisez la fonction train_test_split de sklearn. Stocker les résultats dans des variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eyUut7LPXuHT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3dQzZkgXuHT"
      },
      "source": [
        "<a name='1.4'></a>\n",
        "### 1.4 Construction du vocabulaire (4 points)\n",
        "\n",
        "Comme dans le TP1, construisez un vocabulaire à partir des données d'entraînement. Vous pouvez reprendre votre code du TP1.\n",
        "\n",
        "Complétez la fonction **build_voc** qui retourne une liste de jetons qui sont présents au moins n fois (threshold passé en paramètre) dans la liste d'exemples (également passée en paramètre). Vous pouvez utiliser la classe collections.Counter.\n",
        "\n",
        "Ensuite, appelez cette fonction pour construire votre vocabulaire à partir de l'ensemble d'entraînement **en utilisant threshold=2**. Imprimez la taille du vocabulaire.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sdkMCukdXuHT"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_voc(documents, threshold):\n",
        "\n",
        "    word_counts = Counter()\n",
        "\n",
        "    for sentence in documents:\n",
        "        for word in sentence:\n",
        "            word_counts[word] += 1\n",
        "\n",
        "    vocabulary = [word for word, count in word_counts.items() if count >= threshold]\n",
        "\n",
        "    return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IA0Ll9pkXuHT",
        "outputId": "39c87052-294f-4111-bba2-51bbc77f8e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 5608\n"
          ]
        }
      ],
      "source": [
        "voc = build_voc(train, 2)\n",
        "print(\"Vocabulary size:\", len(voc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aORzF7_fXuHT"
      },
      "source": [
        "<a name='1.5'></a>\n",
        "### 1.5 Mots hors vocabulaire (4 points)\n",
        "\n",
        "Si votre modèle réalise de l'autocomplétion, mais qu'il rencontre un mot qu'il n'a jamais vu lors de l'entraînement, le modèle ne pourra donc pas prédire le mot suivant car il n'y a pas d'occurrence pour le mot actuel.\n",
        "\n",
        "Ces mots sont appelés les mots hors vocabulaire (Out of Vocabulary) <b>OOV</b>.\n",
        "Le pourcentage de mots inconnus dans l'ensemble de test est appelé le taux de mots <b> OOV </b>.\n",
        "\n",
        "Pour gérer les mots inconnus lors de la prédiction, utilisez un jeton spécial 'unk' pour représenter tous les mots inconnus. Plus spécifiquement, la technique que vous utiliserez sera la suivante:\n",
        "\n",
        "Complétez la fonction replace_oov qui convertit tous les mots qui ne font pas partie du vocabulaire en jeton '\\<unk\\>'.\n",
        "\n",
        "Appelez ensuite votre fonction sur votre corpus d'entraînement et de test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QwSKLyiJXuHT"
      },
      "outputs": [],
      "source": [
        "def replace_oov(tokenized_sentences, voc):\n",
        "\n",
        "    tokenized_sentences_processed = []\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        sentence_processed = []\n",
        "        for word in sentence:\n",
        "            sentence_processed.append(word) if word in voc else sentence_processed.append('<unk>')\n",
        "        tokenized_sentences_processed.append(sentence_processed)\n",
        "\n",
        "    return tokenized_sentences_processed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "izmQnVvfXuHT"
      },
      "outputs": [],
      "source": [
        "train = replace_oov(train, voc)\n",
        "test = replace_oov(test, voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vhHqaRXgXuHU",
        "outputId": "cf980c48-f197-4fe5-c404-c6e5069f246f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase initiale:\n",
            "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
            "Phrase segmentée avec'<unk>':\n",
            "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]]\n",
        "vocabulary = build_voc([[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]], 2)\n",
        "tmp_replaced_tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
        "print(f\"Phrase initiale:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"Phrase segmentée avec'<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of0aQey_XuHU"
      },
      "source": [
        "##### Sortie attendue\n",
        "```CPP\n",
        "Phrase initiale:\n",
        "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
        "Phrase segmentée avec '<unk>':\n",
        "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFYUglceXuHU"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. Modèles de langue n-gramme (18 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW5UPpR3XuHU"
      },
      "source": [
        "Dans cette section, vous développerez un modèle de langue n-grammes. Nous allons utiliser la formule:\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "- La fonction $C(\\cdots)$ représente le nombre d'occurrences de la séquence donnée.\n",
        "- $\\hat{P}$ signifie l'estimation de $P$.\n",
        "\n",
        "Vous pouvez estimer cette probabilité en comptant les occurrences de ces séquences de mots dans les données d'entraînement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaewmvwEXuHU"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Fréquence des n-grammes (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQLypfBPXuHU"
      },
      "source": [
        "\n",
        "\n",
        "Vous allez commencer par mettre en œuvre une fonction qui calcule la fréquence des n-grammes pour un nombre arbitraire $n$.\n",
        "\n",
        "Vous devez pré-traiter la phrase en ajoutant $n$ marqueurs de début de phrase \"\\<s\\>\" pour indiquer le commencement de la phrase.\n",
        "\n",
        "- Par exemple, dans un modèle bigramme (N=2), la séquence devrait commencer avec deux jetons de début \"\\<s\\>\\<s\\>\". Ainsi, si la phrase est \"J'aime la nourriture\", modifiez-la pour devenir \"\\<s\\>\\<s\\> J'aime la nourriture\".\n",
        "- Ajoutez aussi un jeton de fin \"\\<e\\>\" pour que le modèle puisse prédire quand terminer une phrase.\n",
        "    \n",
        "\n",
        "Dans cette implémentation, vous devez stocker les occurrences des n-grammes sous forme de dictionnaire.\n",
        "\n",
        "- La clé de chaque paire clé-valeur dans le dictionnaire est un tuple de n mots (et non une liste).\n",
        "- La valeur dans la paire clé-valeur est le nombre d'occurrences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cfUtjqVgXuHU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    # Process the data\n",
        "    for sentence in data:\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "        processed_data.append(sentence)\n",
        "\n",
        "    # Count the number of occurence of each n-gram\n",
        "    n_grams = {}\n",
        "    for sentence in processed_data:\n",
        "        for i in range(len(sentence)-n+1):\n",
        "            n_gram = tuple(sentence[i:i+n])\n",
        "            if n_gram in n_grams:\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QRX5C2IKXuHV",
        "outputId": "91c22125-f848-458a-ef7d-635198940663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigrammes:\n",
            "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
            "Bigrammes:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "print(\"Unigrammes:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bigrammes:\")\n",
        "print(count_n_grams(sentences, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6E8cR0tXuHV"
      },
      "source": [
        "Sortie attendue:\n",
        "\n",
        "```CPP\n",
        "Unigrammes:\n",
        "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
        "Bigrammes:\n",
        "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjeLbAoMXuHV"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 Estimé du maximum de vraisemblance MLE (4 points)\n",
        "\n",
        "#### 2.2.1 Calcul de probabilité pour un mot (3 points)\n",
        "\n",
        "\n",
        "Ensuite, estimez la probabilité d'un mot étant donnés les 'n' mots précédents avec les fréquences obtenues.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2}$$\n",
        "\n",
        "\n",
        "La fonction prend en entrée:\n",
        "\n",
        "- word : le mot dont on veut estimer la probabilité\n",
        "- previous_n_gram : le n-gramme précédent, sous forme de tuple\n",
        "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
        "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XFQhWv_oXuHV"
      },
      "outputs": [],
      "source": [
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts):\n",
        "\n",
        "    # On adapte la taille du previous_n_gram en fonction de la taille de n_gram_counts\n",
        "    len_context = len(next(iter(n_gram_counts)))\n",
        "    if len(previous_n_gram) > len_context:\n",
        "        previous_n_gram = previous_n_gram[-len_context:]\n",
        "    elif len(previous_n_gram) < len_context:\n",
        "        previous_n_gram = (len_context - len(previous_n_gram)) * ('<s>',) + previous_n_gram\n",
        "\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "    n_plus_1_gram = previous_n_gram + (word,)\n",
        "    n_plus_1_gram_count = n_plus1_gram_counts[n_plus_1_gram] if n_plus_1_gram in n_plus1_gram_counts else 0\n",
        "\n",
        "    probability = n_plus_1_gram_count / previous_n_gram_count\n",
        "\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy0KSBMmXuHV"
      },
      "source": [
        "#### 2.2.1 Quel est le problème de cette fonction? Quelle embûche pourrait-on rencontrer? Répondre avec un exemple. (1 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybAxhs0XuHV"
      },
      "source": [
        "> Si on cherche la probabilité d'un mot sachant un n-gramme qui n'appartient pas à notre dictionnaire, alors le dénominateur sera égal à 0. On ne pourra donc pas calculer la probabilité du mot. (Exemple ci-dessous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "cGz3QK0eXuHV",
        "outputId": "ab976205-f338-477b-bee9-be6522b3c128"
      },
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Lucas\\Desktop\\Cours PolyMtl\\INF8460 - TALN\\TP2\\TP2_inf8460_A23.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lucas/Desktop/Cours%20PolyMtl/INF8460%20-%20TALN/TP2/TP2_inf8460_A23.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prob \u001b[39m=\u001b[39m estimate_probability(word\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmouse\u001b[39;49m\u001b[39m'\u001b[39;49m, previous_n_gram\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m<s>\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mI\u001b[39;49m\u001b[39m'\u001b[39;49m), n_gram_counts\u001b[39m=\u001b[39;49mcount_n_grams(sentences, \u001b[39m2\u001b[39;49m), n_plus1_gram_counts\u001b[39m=\u001b[39;49mcount_n_grams(sentences, \u001b[39m3\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas/Desktop/Cours%20PolyMtl/INF8460%20-%20TALN/TP2/TP2_inf8460_A23.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(prob)\n",
            "\u001b[1;32mc:\\Users\\Lucas\\Desktop\\Cours PolyMtl\\INF8460 - TALN\\TP2\\TP2_inf8460_A23.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucas/Desktop/Cours%20PolyMtl/INF8460%20-%20TALN/TP2/TP2_inf8460_A23.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m n_plus_1_gram \u001b[39m=\u001b[39m previous_n_gram \u001b[39m+\u001b[39m (word,)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucas/Desktop/Cours%20PolyMtl/INF8460%20-%20TALN/TP2/TP2_inf8460_A23.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m n_plus_1_gram_count \u001b[39m=\u001b[39m n_plus1_gram_counts[n_plus_1_gram] \u001b[39mif\u001b[39;00m n_plus_1_gram \u001b[39min\u001b[39;00m n_plus1_gram_counts \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lucas/Desktop/Cours%20PolyMtl/INF8460%20-%20TALN/TP2/TP2_inf8460_A23.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m probability \u001b[39m=\u001b[39m n_plus_1_gram_count \u001b[39m/\u001b[39;49m previous_n_gram_count\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lucas/Desktop/Cours%20PolyMtl/INF8460%20-%20TALN/TP2/TP2_inf8460_A23.ipynb#X44sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m probability\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "prob = estimate_probability(word='mouse', previous_n_gram=('<s>', 'I'), n_gram_counts=count_n_grams(sentences, 2), n_plus1_gram_counts=count_n_grams(sentences, 3))\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPIMIhBCXuHV"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3  Lissage add-k (4 points)\n",
        "\n",
        "Vous allez maintenant modifier votre fonction précédente en utilisant le lissage add-k.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
        "\n",
        "Recodez la fonction au numéro 2.2 en ajoutant une constante de lissage $k$ and la taille du vocabulaire en paramètres supplémentaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sGbsryf-XuHV"
      },
      "outputs": [],
      "source": [
        "def estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "    # On adapte la taille du previous_n_gram en fonction de la taille de n_gram_counts\n",
        "    len_context = len(next(iter(n_gram_counts)))\n",
        "    if len(previous_n_gram) > len_context:\n",
        "        previous_n_gram = previous_n_gram[-len_context:]\n",
        "    elif len(previous_n_gram) < len_context:\n",
        "        previous_n_gram = (len_context - len(previous_n_gram)) * ('<s>',) + previous_n_gram\n",
        "\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "    n_plus_1_gram = previous_n_gram + (word,)\n",
        "    n_plus_1_gram_count = n_plus1_gram_counts[n_plus_1_gram] if n_plus_1_gram in n_plus1_gram_counts else 0\n",
        "\n",
        "    probability = (n_plus_1_gram_count + k) / (previous_n_gram_count + k*vocabulary_size)\n",
        "\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NSitwnTMXuHW",
        "outputId": "0a1c23b5-d5f8-49c8-d3fb-e2688012c8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "tmp_prob = estimate_probability_smoothing(\"have\", ('<s>', 'i',), bigram_counts, trigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\" La probabilité de 'have' étant donné le mot précédent 'i' est: {tmp_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFFuDQMDXuHW"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIPbQ5ttXuHW"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 Calcul des probabilités des n-grammes (7 points)\n",
        "\n",
        "#### 2.4.1. Estimation des probabilités (4 points)\n",
        "Complétez la fonction estimate_probabilities qui calcule pour chaque mot du vocabulaire la probabilité d'être généré en utilisant la fonction avec lissage add-k.\n",
        "\n",
        "N'oubliez pas d'ajouter le jetons spécial \"\\<e\\>\" au vocabulaire\n",
        "\n",
        "Cette fonction prends en entrée:\n",
        "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
        "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
        "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
        "- vocabulary: le vocabulaire\n",
        "- k: la constante de lissage\n",
        "\n",
        "La fonction retourne un dictionnaire ayant pour clés tous les mots du vocabulaire ainsi que leur probabilité d'être générés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YjRc0hfAXuHW"
      },
      "outputs": [],
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "\n",
        "    # On ajoute le token de fin de phrase s'il n'est pas déjà présent ou a été oublié\n",
        "    vocabulary = set(vocabulary).union(set(['<e>']))\n",
        "\n",
        "    words_probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        words_probabilities[word] = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k)\n",
        "\n",
        "    return words_probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5zaJP1hVXuHZ",
        "outputId": "ad84af2b-94d8-42f2-e6c7-c5add6fa7b16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cats': 0.1111111111111111,\n",
              " 'mouse': 0.2222222222222222,\n",
              " 'this': 0.1111111111111111,\n",
              " '<e>': 0.1111111111111111,\n",
              " 'i': 0.1111111111111111,\n",
              " 'likes': 0.1111111111111111,\n",
              " 'have': 0.1111111111111111,\n",
              " 'a': 0.1111111111111111}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "estimate_probabilities((\"a\",), unigram_counts, bigram_counts, unique_words, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22v6ssSGXuHa"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "{'likes': 0.1111111111111111,\n",
        " 'have': 0.1111111111111111,\n",
        " 'this': 0.1111111111111111,\n",
        " 'i': 0.1111111111111111,\n",
        " 'mouse': 0.2222222222222222,\n",
        " 'a': 0.1111111111111111,\n",
        " 'cats': 0.1111111111111111,\n",
        " '<e>': 0.1111111111111111}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxyldcaGXuHa"
      },
      "source": [
        "#### 2.4.2. Probabilités étant donné un contexte (3 points)\n",
        "\n",
        "Affichez maintenant les probabilités des tri-grammes étant donné le context \"i will\" en utilisant les données d'entraînement . N'affichez que les 10 mots les plus probables en ordre décroissant de probabilité. Utilisez K=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "txDO3PEuXuHa",
        "outputId": "19ac0711-6eda-476b-e02c-83d5ad0e14b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('tell', 0.008265856950067476), ('fight', 0.005566801619433198), ('fix', 0.0052294197031039135), ('be', 0.004554655870445344), ('never', 0.004048582995951417), ('say', 0.00354251012145749), ('not', 0.0021929824561403508), ('ask', 0.0020242914979757085), ('work', 0.0016869095816464238), ('also', 0.0016869095816464238)]\n"
          ]
        }
      ],
      "source": [
        "unigram_counts = count_n_grams(train, 1)\n",
        "bigram_counts = count_n_grams(train, 2)\n",
        "trigram_counts = count_n_grams(train, 3)\n",
        "\n",
        "voc = build_voc(train, 2)\n",
        "voc.append('<e>')\n",
        "\n",
        "probabilities = estimate_probabilities((\"i\", \"will\",), bigram_counts, trigram_counts , voc, k=1)\n",
        "probabilities_sorted = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(probabilities_sorted[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjca-ut4XuHa"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "[('tell', 0.0070981916511745815),\n",
        " ('fix', 0.005239141456819334),\n",
        " ('fight', 0.005239141456819334),\n",
        " ('be', 0.0050701368936961295),\n",
        " ('never', 0.004225114078080108),\n",
        " ('say', 0.003718100388710495),\n",
        " ('not', 0.0025350684468480648),\n",
        " ('ask', 0.0023660638837248605),\n",
        " ('also', 0.0018590501943552475),\n",
        " ('work', 0.001521041068108839)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpmcwCIZXuHa"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Perplexité (15 points)\n",
        "\n",
        "Dans cette section, vous allez générer le score de perplexité pour évaluer votre modèle sur l'ensemble de test.\n",
        "\n",
        "Pour calculer le score de perplexité d'une phrase sur un modèle n-gramme, utilisez :\n",
        "\n",
        "$$PP(W) =\\sqrt[N]{ \\prod_{t=1}^{N} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n",
        "\n",
        "où N = le nombre de jeton dans la phrases incluant le jeton \\<e\\>\n",
        "et P = la probabilité de générer le jeton $w_t$\n",
        "\n",
        "Plus les probabilités sont élevées, plus la perplexité sera basse.\n",
        "\n",
        "<a name='3.1'></a>\n",
        "### 3.1. Calcul de la perplexité (4 points)\n",
        "Complétez la fonction `calculate_perplexity`, qui pour une phrase donnée, nous donne le score de perplexité. Cette fonction prend en entrée:\n",
        "\n",
        "\n",
        "- sentence: La phrase pour laquelle vous devez calculer la perplexité\n",
        "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
        "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
        "- vocabulary_size: la taille du vocabulaire\n",
        "- k: la constante de lissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Qj9YcZH1XuHa"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "    # Le nombre de jetons (on ajoute 1 pour le token de fin de phrase)\n",
        "    N = len(sentence) + 1\n",
        "    # On récupère la taille du n-gramme\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    proba_total = 1\n",
        "\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "    for i in range(n, N):\n",
        "        n_gram = tuple(sentence[i-n:i])\n",
        "        word = sentence[i]\n",
        "        proba_total *= 1 / estimate_probability_smoothing(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
        "\n",
        "    perplexity = proba_total**(1/N)\n",
        "\n",
        "    return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "S72u6fdZXuHb",
        "outputId": "34262344-ab57-45d7-f545-d3bcc06189aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité de la première phrase: 3.4019\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1] + ['<e>']))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "\n",
        "perplexity = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexité de la première phrase: {perplexity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkbPDWY4XuHb"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2. Perplexité sur une phrase d'entraînement (4 points)\n",
        "Calculez et affichez la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes à l'aide de votre fonction `calculate_perplexity` définie plus haut sur la première phrase de votre corpus d'entraînement. Utilisez K=0.01 ici."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bhwZ2iu6XuHb",
        "outputId": "28e68634-87b7-4ad4-bf43-c8adef49fe23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du modèle bi-gramme :\n",
            "23.159151227367058\n",
            "Perplexité du modèle tri-gramme :\n",
            "20.336762114542754\n",
            "Perplexité du modèle quadri-gramme :\n",
            "17.122946540155546\n"
          ]
        }
      ],
      "source": [
        "bigram_counts = count_n_grams(train, 2)\n",
        "trigram_counts = count_n_grams(train, 3)\n",
        "quadrigram_counts = count_n_grams(train, 4)\n",
        "quintigram_counts = count_n_grams(train, 5)\n",
        "\n",
        "print(f\"Perplexité du modèle bi-gramme :\")\n",
        "print(calculate_perplexity(train[0], bigram_counts, trigram_counts, len(voc), k=0.01))\n",
        "\n",
        "print(f\"Perplexité du modèle tri-gramme :\")\n",
        "print(calculate_perplexity(train[0], trigram_counts, quadrigram_counts, len(voc), k=0.01))\n",
        "\n",
        "print(f\"Perplexité du modèle quadri-gramme :\")\n",
        "print(calculate_perplexity(train[0], quadrigram_counts, quintigram_counts, len(voc), k=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmJVOJ5-XuHb"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3. Perplexité du corpus de test (7 points)\n",
        "\n",
        "#### 3.3.1. Vous pouvez maintenant calculer et afficher la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes sur votre corpus de test. K=1 ici. (4 points)\n",
        "\n",
        "Pour calculer la perplexité d'un corpus de *m* phrases, il suffit de suivre la formule suivante :\n",
        "\n",
        "Soit $N$ le nombre total de jetons dans le corpus de test C et $N_i$ le nombre de jetons dans la phrase i.\n",
        "\n",
        "$$Perplexity(C) = \\Big(\\frac{1}{P(s_1, ..., s_m)}\\Big)^{1/N}$$\n",
        "$$P(s_1, ..., s_m) = \\prod_{i=1}^{m} p(s_i)$$\n",
        "$$p(s_i) = \\prod_{t=1}^{N_i} \\hat{P}(w_t | w_{t-n} \\cdots w_{t-1})$$\n",
        "\n",
        "Puisqu'il s'agit d'un multiplication de probabilités (situées entre 0 et 1), le produit devient nul très rapidement. C'est pourquoi il est plus efficace d'effectuer une transformation vers un espace logarithmique pour transformer les multiplications en addition. Cela donne ainsi la formule suivante:\n",
        "\n",
        "$$LogPerplexity(C) = 2^{-\\frac{1}{N} \\sum_{k=1}^{m} log_{2} \\; p(s_k)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YiYfc0AvXuHc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_perplexity_corpus(corpus, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "    # n est la taille du n-gramme\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    # Compter les jetons du corpus\n",
        "    N = 0\n",
        "    for sentence in corpus:\n",
        "        N += len(sentence) + 1 # +1 pour le token de fin de phrase\n",
        "\n",
        "    somme_log = 0\n",
        "\n",
        "    for sentence in corpus:\n",
        "        sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "        p_s = 1\n",
        "        for i in range(n, len(sentence)):\n",
        "            n_gram = tuple(sentence[i-n:i])\n",
        "            word = sentence[i]\n",
        "            p_s *= estimate_probability_smoothing(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
        "        somme_log += np.log2(p_s)\n",
        "\n",
        "    log_perplexity = -somme_log/N\n",
        "\n",
        "    return 2**log_perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "y64Nba6HXuHc",
        "outputId": "fe3e9b70-7719-4255-c237-977947b451e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du corpus de test: 7.708920690856638\n"
          ]
        }
      ],
      "source": [
        "n_gram_counts = {('<s>', 'quick'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
        "n_plus1_gram_counts = { ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
        "\n",
        "train_corpus = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
        "n_gram_counts = {('<s>', '<s>'): 2, ('<s>', 'the'): 1, ('<s>', 'jumps'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', '<e>'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
        "n_plus1_gram_counts = {('<s>', '<s>', '<s>', ): 2, ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1,  ('<s>', '<s>', 'jumps', ): 1, ('<s>', 'jumps', 'over'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', '<e>'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
        "\n",
        "vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"<e>\"]\n",
        "\n",
        "N = 5\n",
        "V = len(vocabulary)\n",
        "\n",
        "test_corpus = [[\"the\", \"fox\"], [\"jumps\"]]\n",
        "\n",
        "# Complétez le calcul de la perplexité avec k=1\n",
        "\n",
        "perplexity = calculate_perplexity_corpus(test_corpus, n_gram_counts, n_plus1_gram_counts, V, k=1)\n",
        "print(f\"Perplexité du corpus de test: {perplexity}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e-xmO4iXuHc"
      },
      "source": [
        "#### Sortie attendue\n",
        "\n",
        "    Perplexité du corpus de test:  7.708920690856638"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4rrzukzsXuHc",
        "outputId": "d8fd7a99-40e2-4d0d-8206-d174371ecbca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du corpus de test avec le modèle bi-gramme :\n",
            "208.63191684235457\n",
            "Perplexité du corpus de test avec le modèle tri-gramme :\n",
            "490.425193857908\n",
            "Perplexité du corpus de test avec le modèle quadri-gramme :\n",
            "750.6077767312561\n"
          ]
        }
      ],
      "source": [
        "# Calculez mainenant la perplexité de votre corpus de test\n",
        "bigram_counts = count_n_grams(train, 2)\n",
        "trigram_counts = count_n_grams(train, 3)\n",
        "quadrigram_counts = count_n_grams(train, 4)\n",
        "quintgram_counts = count_n_grams(train, 5)\n",
        "\n",
        "voc = build_voc(train, 2)\n",
        "voc.append('<e>')\n",
        "\n",
        "print(f\"Perplexité du corpus de test avec le modèle bi-gramme :\")\n",
        "print(calculate_perplexity_corpus(test, bigram_counts, trigram_counts, len(voc), k=0.01))\n",
        "\n",
        "print(f\"Perplexité du corpus de test avec le modèle tri-gramme :\")\n",
        "print(calculate_perplexity_corpus(test, trigram_counts, quadrigram_counts, len(voc), k=0.01))\n",
        "\n",
        "print(f\"Perplexité du corpus de test avec le modèle quadri-gramme :\")\n",
        "print(calculate_perplexity_corpus(test, quadrigram_counts, quintgram_counts, len(voc), k=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VtQPZyAXuHc"
      },
      "source": [
        "#### 3.3.2. Les perplexités attendues peuvent sembler contre-intuitives.  Comparez-les aux perplexités obtenues sur l'ensemble d'entrainement pour les mêmes modèles. Comment expliquez-vous ces résultats et quelle est votre conclusion ?  (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SkQHorIv9qQ"
      },
      "source": [
        "Sur le corpus d'entraînement :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "NB-4jl_3v9Z3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du corpus de test avec le modèle bi-gramme :\n",
            "33.04071284302768\n",
            "Perplexité du corpus de test avec le modèle tri-gramme :\n",
            "37.54620559913497\n",
            "Perplexité du corpus de test avec le modèle quadri-gramme :\n",
            "41.45555774085327\n"
          ]
        }
      ],
      "source": [
        "print(f\"Perplexité du corpus de test avec le modèle bi-gramme :\")\n",
        "print(calculate_perplexity_corpus(train, bigram_counts, trigram_counts, len(voc), k=0.01))\n",
        "\n",
        "print(f\"Perplexité du corpus de test avec le modèle tri-gramme :\")\n",
        "print(calculate_perplexity_corpus(train, trigram_counts, quadrigram_counts, len(voc), k=0.01))\n",
        "\n",
        "print(f\"Perplexité du corpus de test avec le modèle quadri-gramme :\")\n",
        "print(calculate_perplexity_corpus(train, quadrigram_counts, quintgram_counts, len(voc), k=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuMEsVE6XuHc"
      },
      "source": [
        "On remarque que la perplexité augmente avec la taille des n-grammes. Cela peut s'expliquer par le besoin croissant de taille de corpus avec la longueur des n-grammes : plus les n-grammes sont longs, plus les mots hors-vocabulaire seront courants. La perplexité se basant sur les probabilités des n-grammes, plus les n-grammes sont longs, et plus ces probabilités auront tendance à être faibles car auront moins été vues. Il n'est donc pas cohérent de comparer des perplexités de n-grammes de longueur différentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qpK-7saXuHc"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Construction d'un modèle d'auto-complétion (15 points)\n",
        "\n",
        "Dans cette dernière partie, vous allez utiliser les modèles n-grammes construits aux numéros précédents afin de faire un modèle d'autocomplétion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43JMgH5BXuHd"
      },
      "source": [
        "<a name='4.1'></a>\n",
        "### 4.1 Suggestion d'un mot à partir d'un préfixe (5 points)\n",
        "\n",
        "\n",
        "La première étape sera de construire une fonction qui suggère un mot à partir des premiers caractères entrés par un utilisateur, considérant un seul type de n-gramme.  \n",
        "\n",
        "Complétez la fonction `suggest_word` qui calcule les probabilités pour tous les mots suivants possibles et suggère le mot le plus probable. Comme contrainte supplémentaire, le mot suggéré doit commencer avec le préfixe passé en paramètre. Utilisez vos fonctions provenant du numéro 2. (Modèle n-gramme de mots) pour faire vos prédictions.\n",
        "\n",
        "Cette fonction prends en paramètre:\n",
        "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
        "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
        "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
        "- vocabulary_size: la taille du vocabulaire\n",
        "- k: la constante de lissage\n",
        "- prefixe: Le début du mot que l'on veut prédire\n",
        "\n",
        "Elle retourne le mot le plus probable avec la probabilité associée"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XgCnQnwHXuHd"
      },
      "outputs": [],
      "source": [
        "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, prefixe=\"\"):\n",
        "\n",
        "    # On ajoute le token de fin de phrase s'il n'est pas déjà présent ou a été oublié\n",
        "    vocabulary = set(vocabulary).union(set(['<e>']))\n",
        "\n",
        "    # La gestion de la taille du contexte est déjà gérée dans estimate_probability_smoothing\n",
        "    probabilities = estimate_probabilities(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k)\n",
        "\n",
        "    # On trie les mots par probabilité décroissante\n",
        "    probabilities_sorted = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # On récupère le mot le plus probable commençant par le préfixe\n",
        "    for word, prob in probabilities_sorted:\n",
        "        if word.startswith(prefixe):\n",
        "            return word, prob\n",
        "\n",
        "    return \"\", 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oKb_Z3UTXuHd",
        "outputId": "3828593b-322f-43f9-f27e-031b580107f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " avec les mots précédents 'i have',\n",
            "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
            "\n",
            "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
            "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = (\"i\", \"have\",)\n",
        "tmp_suggest1 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\" avec les mots précédents 'i have',\\n\\t le mot suggéré est `{tmp_suggest1[0]}` avec la probabilité {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "tmp_starts_with = 'm'\n",
        "tmp_suggest2 = suggest_word(previous_tokens, bigram_counts, trigram_counts, unique_words, k=1.0, prefixe=tmp_starts_with)\n",
        "print(f\"avec les mots précédents 'i have', et une suggestion qui commence par `{tmp_starts_with}`\\n\\t le mot suggéré est : `{tmp_suggest2[0]}` avec une probabilité de {tmp_suggest2[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRw8Gc--XuHd"
      },
      "source": [
        "### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "avec les mots précédents 'i have',\n",
        "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
        "\n",
        "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
        "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ_mp2lvXuHd"
      },
      "source": [
        "<a name='4.2'></a>\n",
        "### 4.2 Suggestions multiples (5 points)\n",
        "\n",
        "Afin de suggérer plusieurs mots à l'utilisateur, une stratégie que l'on peut utiliser est de retourner un ensemble de mots suggérés par plusieurs types de modèles n-grammes.\n",
        "\n",
        "En utilisant la fonction `suggest_word` du numéro précédent, complétez la fonction `get_suggestions` qui retourne les suggestions des modèles n-grammes passés en paramètre. Vous devrez aussi enlever les doublons dans les suggestions s'il y en a, et ordonner la liste des suggestions en commençant par le mot ayant la probabilité la plus élevée.\n",
        "\n",
        "La fonction get_suggestions prends en paramètres:\n",
        "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
        "- n_gram_counts_list: une liste de n-grammes dans l'ordre suivant [unigrammes, bigrammes, trigrammes, quadrigrammes, ...]\n",
        "- vocabulary_size: la taille du vocabulaire\n",
        "- k: la constante de lissage (entre 0 et 1)\n",
        "- prefixe: Le début du mot que l'on veut prédire, \"\" si au aucun préfixe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_px5LQJFXuHd"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, prefixe=\"\"):\n",
        "\n",
        "    suggested_word_list = []\n",
        "\n",
        "    # La gestion de la taille du previous_tokens est faite dans la fonction estimate_probabilitiy_smoothing\n",
        "    for i in range(len(n_gram_counts_list)-1):\n",
        "        suggested_word_list.append(suggest_word(previous_tokens, n_gram_counts_list[i], n_gram_counts_list[i+1], vocabulary, k, prefixe))\n",
        "\n",
        "    suggested_word_dict = {}\n",
        "\n",
        "    # Enlève les doublons et garde la plus grande probabilité de chaque suggestion si un mot est suggéré par plusieurs modèles\n",
        "    for word, prob in suggested_word_list:\n",
        "        if word in suggested_word_dict:\n",
        "            if prob > suggested_word_dict[word]:\n",
        "                suggested_word_dict[word] = prob\n",
        "        else:\n",
        "            suggested_word_dict[word] = prob\n",
        "\n",
        "    # Trie les suggestions par probabilité décroissante\n",
        "    suggested_word = sorted(suggested_word_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # On ne garde que les mots\n",
        "    suggested_word = [word for word, _ in suggested_word]\n",
        "\n",
        "    return suggested_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "sA_ofSw1XuHd",
        "outputId": "fc79698a-418c-4552-dada-ffe5196e4bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Etant donné les mots i have, je suggère :\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['a']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1] + ['<e>']))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "quadgram_counts = count_n_grams(sentences, 4)\n",
        "qintgram_counts = count_n_grams(sentences, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "previous_tokens = (\"i\", \"have\",)\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
        "\n",
        "print(f\"Etant donné les mots i have, je suggère :\")\n",
        "display(tmp_suggest3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vI7FbxYXuHd"
      },
      "source": [
        "##### Sortie attendue\n",
        "\n",
        "```CPP\n",
        "Etant donné les mots i have, je suggère :\n",
        "['a']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeMszMcBXuHe"
      },
      "source": [
        "<a name='4.3'></a>\n",
        "### 4.3 Autocomplétion (5 points)\n",
        "\n",
        "Il est maintenant temps de combiner vos fonctions afin de créer le modèle d'autocomplétion. En utilisant le jeu de données d'entraînement, calculez la fréquence des n-grammes allant de 1 à 5 et utilisez la fonction *get_suggestions* afin de suggérer des mots. Vous devrez être en mesure de toujours suggérer des mots à partir du dernier mot entré par l'utilisateur.\n",
        "\n",
        "Complétez la fonction *update_suggestions*:\n",
        "- la variable texte_actuel contient tout le texte entré par l'utilisateur\n",
        "- la variable top_suggestions contient les suggestions qui seront proposées\n",
        "\n",
        "Vous devrez changer le contenu de la variable top_suggestions pour qu'elle contienne les suggestions des n-grammes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0eRRM6cbXuHe",
        "outputId": "a763811e-4739-49e8-99d7-6c87f50e5ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (8.1.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (8.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (5.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.9 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (4.0.9)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (3.0.9)\n",
            "Requirement already satisfied: backcall in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\lucas\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Users\\Lucas\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9a3230ee660946bcba6a066fb797cd2e",
            "0b65da9ab6db43ecb14ca57895c9822d"
          ]
        },
        "id": "0LTnRtShXuHe",
        "outputId": "73031d22-ba79-4e12-bdf1-9a6fa8e0089d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97a25e7fc9044c56b2c0300989f47221",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', placeholder='Entrez votre text ici...')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b496841dbf4406f90fa064492a9bad1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Label(value='Suggestions: ')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "text_input = widgets.Text(placeholder=\"Entrez votre text ici...\")\n",
        "\n",
        "suggestions_label = widgets.Label(value=\"Suggestions: \")\n",
        "\n",
        "# Génère les n-grammes pour le modèle\n",
        "unigram_counts = count_n_grams(train, 1)\n",
        "bigram_counts = count_n_grams(train, 2)\n",
        "trigram_counts = count_n_grams(train, 3)\n",
        "quadgram_counts = count_n_grams(train, 4)\n",
        "qintgram_counts = count_n_grams(train, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "\n",
        "def update_suggestions(change):\n",
        "    texte_actuel = change[\"new\"]\n",
        "    \n",
        "    # On préprocess le texte\n",
        "    texte = texte_actuel.lower()\n",
        "    texte = texte.replace('\\n', ' ')\n",
        "    texte_without_space = re.sub(r'[^\\w\\s]', '', texte)\n",
        "    texte_without_space = nltk.word_tokenize(texte_without_space) # Tableau des mots du texte sans les espaces\n",
        "    texte_processed = replace_oov([texte_without_space], voc) # Tableau des mots du texte avec les mots inconnus remplacés par <unk>\n",
        "\n",
        "    texte_splitted = texte.split(\" \") # Tableau des mots du texte. texte_splitted[-1] contient un mot vide si le dernier caracère tapé est un espace\n",
        "\n",
        "    # Calcul du prefixe et des tokens précédents \n",
        "    if texte_splitted[-1] == \"\": # Si le dernier caractère est un espace, on fixe le préfixe à \"\" et on prend les 4 derniers mots comme tokens \n",
        "        prefixe = \"\"\n",
        "        previous_tokens = tuple(texte_processed[0][-4:]) # On prend les 4 derniers mots comme tokens\n",
        "    else:\n",
        "        prefixe = texte_without_space[-1] # On prend le dernier mot comme préfixe (qui est celui en cours d'écriture)\n",
        "        if len(texte_processed[0]) >= 5: # Si le texte contient au moins 5 mots (celui en cours d'écriture + 4 mots précédents), on prend les 4 précédents comme tokens\n",
        "            previous_tokens = tuple(texte_processed[0][-5:-1])\n",
        "        else: # Sinon on prend tous les mots précédents à celui en cours d'écriture\n",
        "            previous_tokens = tuple(texte_processed[0][:-1])\n",
        "    \n",
        "    # On récupère les suggestions\n",
        "    if len(texte_processed) == 0:\n",
        "        top_suggestions = []\n",
        "    else:\n",
        "        top_suggestions = get_suggestions(previous_tokens, n_gram_counts_list, voc, k=1.0, prefixe=prefixe)\n",
        "        suggestions_label.value = \"Suggestions: \" + \", \".join(top_suggestions)\n",
        "\n",
        "text_input.observe(update_suggestions, names=\"value\")\n",
        "\n",
        "display(text_input)\n",
        "display(suggestions_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T40jnvcEXuHe"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Modèle de génération de phrases (30 points)\n",
        "\n",
        "Dans cette partie vous allez construire un modèle de génération de phrases en utilisant les n-grammes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewYOEQJ9XuHe"
      },
      "source": [
        "#### Dans la cadre d'un modèle de génération de phrases, indiquez pourquoi la stratégie de suggestion des mots en 4. ne peut pas fonctionner ? (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXsBt_YLXuHe"
      },
      "source": [
        "Notre modèle d'auto-complétion suggère un mot à la fois basé sur le n-gramme précédent. Donc les propositions seront toujours identiques pour un même n-gramme précédent. (et notamment pour le premier mot où le n-gramme précédent est toujours \\<s> et donc les suggestions du premier mot seront toujours les memes)  \n",
        "  \n",
        "Ainsi, ce modèle ne permet pas de générer des phrases car il propose toujours les memes mots pour un meme contexte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4AqBKi2XuHe"
      },
      "source": [
        "<a name='5.1'></a>\n",
        "\n",
        "### 5.1 Génération stochastique de mots (5 points)\n",
        "\n",
        "Recodez la fonction suggest_word afin d'utiliser une suggestion stochastique. Autrement dit, au lieu de retourner le mot le plus probable, vous devrez générez le mot suivant selon sa probabilité.\n",
        "\n",
        "Par exemple si le mot 'like' a la probabilité 0.25 d'être généré, alors il sera retourné 25% du temps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "1WOlW4FfXuHe"
      },
      "outputs": [],
      "source": [
        "def suggest_word_with_probs(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    probabilities = estimate_probabilities(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k)\n",
        "\n",
        "    random_word = np.random.choice(list(probabilities.keys()), p=list(probabilities.values()))\n",
        "    probability = probabilities[random_word]\n",
        "\n",
        "    return random_word, probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEoTBkUMXuHf"
      },
      "source": [
        "<a name='5.2'></a>\n",
        "### 5.2 Générations de phrases (10 points)\n",
        "\n",
        "#### 5.2.1. Génération stochastique (4 points)\n",
        "Complétez maintenant la fonction `generate_sentence` qui génère une phrase longue de n_words en appelant votre nouvelle fonction `suggest_words_with_probs`. La génération doit s'arrêter si le modèle génère un jeton de fin de phrase.\n",
        "\n",
        "Il ne faut pas oublier d'initialiser les phrases à générer avec le bon nombre de jetons de début de phrase (`<s>`). Par exemple, s'il s'agit d'un modèle bigramme, il faudra initialiser la phrase à [`<s>`]. S'il s'agit d'un modèle trigramme, il faudra initialiser la phrase à [`<s>`, `<s>`]. Vous pouvez trouver la taille du contexte à l'aide de l'expression suivante `len(next(iter(n_gram_counts)))`.\n",
        "\n",
        "Ensuite, il faudra passer à la fonction `suggest_word` les `n` derniers mots générés où `n` correspond à la taille du contexte.\n",
        "Finalement, il faudra arrêter la génération si le jeton généré est le jeton de fin (`<e>`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "I3Pm3_mpXuHf"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.001):\n",
        "\n",
        "    len_context = len(next(iter(n_gram_counts)))\n",
        "\n",
        "    sentence = [\"<s>\"] * len_context\n",
        "    sentence_finished = False\n",
        "\n",
        "    while not sentence_finished:\n",
        "        previous_n_gram = tuple(sentence[-len_context:])\n",
        "        word, prob = suggest_word_with_probs(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k)\n",
        "        sentence.append(word)\n",
        "        if word == \"<e>\" or len(sentence) - len_context >= n_words:\n",
        "            sentence_finished = True\n",
        "\n",
        "    return \" \".join(sentence[len_context:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn0pcZp1XuHf"
      },
      "source": [
        "#### 5.2.2. Test sur des n-grammes (2 points)\n",
        "Testez ensuite votre fonction avec des trigrammes et des 5-grammes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "2K38luOsXuHf"
      },
      "outputs": [],
      "source": [
        "unigram_counts = count_n_grams(train, 1)\n",
        "bigram_counts = count_n_grams(train, 2)\n",
        "trigram_counts = count_n_grams(train, 3)\n",
        "quadrigram_counts = count_n_grams(train, 4)\n",
        "quintigram_counts = count_n_grams(train, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "nQG96iW7XuHf",
        "outputId": "1bfd8308-5a20-414a-86f5-cc923237ae43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée avec le modèle tri-gramme :\n",
            "we want <e>\n",
            "\n",
            "Phrase générée avec le modèle 5-gramme :\n",
            "so i hope lane basics planning friday impressed fellows drawn\n"
          ]
        }
      ],
      "source": [
        "print(\"Phrase générée avec le modèle tri-gramme :\")\n",
        "print(generate_sentence(10, bigram_counts, trigram_counts, voc))\n",
        "print()\n",
        "print(\"Phrase générée avec le modèle 5-gramme :\")\n",
        "print(generate_sentence(10, quadrigram_counts, quintigram_counts, voc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xucOAwEXuHf"
      },
      "source": [
        "#### 5.2.3. Avec k=1.0, que se passe-t-il avec les phrases générées et quelle en est la raison principale ? Que pouvez-vous faire pour améliorer la situation? (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "6LdYDgnEXuHf",
        "outputId": "247e9f93-6e89-4c83-ca7d-6328c9fd0dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée avec le modèle tri-gramme :\n",
            "were lynch scene quite 4000 hostility can undermined system ludicrous\n",
            "\n",
            "Phrase générée avec le modèle 5-gramme :\n",
            "air campaigned engineer constitution cross paying guys projects creating email\n"
          ]
        }
      ],
      "source": [
        "print(\"Phrase générée avec le modèle tri-gramme :\")\n",
        "print(generate_sentence(10, bigram_counts, trigram_counts, voc, k=1))\n",
        "print()\n",
        "print(\"Phrase générée avec le modèle 5-gramme :\")\n",
        "print(generate_sentence(10, quadrigram_counts, quintigram_counts, voc, k=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKXgQSjrXuHf"
      },
      "source": [
        "Avec une valeur de k trop élevée, les phrases générees n'ont pas de sens. Cela est dû au lissage qui est trop important et qui fait que les probabilités de suite de mots n'ayant aucune corrélations sont trop élévées.\n",
        "Pour améliorer la situation, on peut diminuer la valeur de k jusqu'à ce que les phrases générées soient plus cohérentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-zw8o7SXuHf"
      },
      "source": [
        "#### 5.2.4.  Quels sont les problèmes si la constante k a une valeur trop petite, voir 0?  (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oCbDglb_XuHg",
        "outputId": "7d620baa-ec07-4802-b796-1cddc9dea8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée avec le modèle tri-gramme :\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "more than one in four years its going to fight\n",
            "\n",
            "Phrase générée avec le modèle 5-gramme :\n",
            "general mike flynn <e>\n"
          ]
        }
      ],
      "source": [
        "print(\"Phrase générée avec le modèle tri-gramme :\")\n",
        "print(generate_sentence(10, bigram_counts, trigram_counts, voc, k=0.00000001))\n",
        "print()\n",
        "print(\"Phrase générée avec le modèle 5-gramme :\")\n",
        "print(generate_sentence(10, quadrigram_counts, quintigram_counts, voc, k=0.00000001))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r5a7HUxXuHg"
      },
      "source": [
        "Avec une valeur de k très faible, les phrases générées sont proches de celles du corpus d'entrainement. Certaines sont mêmes identiques (par exemple : we will be the big tent party <e>)\n",
        "En effet, avec une valeur de k très faible, il n'y a quasiment aucun lissage et donc certaines suites de mots existant dans le corpus d'entrainement auront des probabilités élévées d'être générés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX9Mq__-XuHg"
      },
      "source": [
        "<a name='5.3'></a>\n",
        "### 5.3. Amélioration de la génération stochastique de mots (12 points)\n",
        "\n",
        "#### 5.3.1. Amélioration stochastique\n",
        "\n",
        "Comme vous avez pu l'observer, la génération stochastique, bien qu'elle soit efficace pour générer des phrases différentes, a tendance à ne pas générer des phrases toujours cohérentes. Proposez une amélioration de la méthode `suggest_word` que vous implémenterez dans la méthode `suggest_word_new` permettant de générer des phrases plus cohérentes.\n",
        "\n",
        "##### a) Décrivez votre méthode dans la cellule suivante (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRWw2VGcXuHg"
      },
      "source": [
        "On propose d'introduire un paramètre t qui va permettre de jouer sur le caractère stochastique de la génération.  \n",
        "Pour cela, on va calculer les probabilités de la même manière que précédemment, mais on va ensuite les modifier en les élévant à la puissance 1/t.  \n",
        "  \n",
        "Donc si t est proche de 1, les probabilités ne seront pas modifiées et la génération sera stochastique.  \n",
        "Si t est plus faible, 1/t sera plus grand et cela aura pour effet d'augmenter les ecart entre les probabilités. Ainsi, les mots les plus probables seront encore plus probables et les mots les moins probables seront encore moins probables.  \n",
        "\n",
        "On pensera à normaliser les probabilités après avoir appliqué la puissance 1/t pour que la somme des probabilités soit toujours égale à 1.  \n",
        "\n",
        "Ainsi, on pourra jouer sur le caractère stochastique de la génération en faisant varier ce paramètre t pour garder une part de génération \"aléatoire\" tout en gardant la cohérence des phrases générées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWIDO76CXuHg"
      },
      "source": [
        "##### b) Implémentez la méthode proposée (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1n14oUKKXuHg"
      },
      "outputs": [],
      "source": [
        "def suggest_word_new(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, t=0.8):\n",
        "\n",
        "    probabilities = estimate_probabilities(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k)\n",
        "\n",
        "    # On applique la puissance 1/t aux probabilités\n",
        "    probabilities = {word: prob**(1/t) for word, prob in probabilities.items()}\n",
        "    # On normalise les probabilités\n",
        "    sum_probabilities = sum(probabilities.values())\n",
        "    probabilities = {word: prob/sum_probabilities for word, prob in probabilities.items()}\n",
        "\n",
        "    # On tire un mot au hasard en fonction des probabilités\n",
        "    random_word = np.random.choice(list(probabilities.keys()), p=list(probabilities.values()))\n",
        "    probability = probabilities[random_word]\n",
        "\n",
        "    return random_word, probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhk1U82XXuHg"
      },
      "source": [
        "#### 5.3.2. Génération améliorée (2 points)\n",
        "Recodez maintenant la fonction `generate_sentence_new` pour appeler votre nouvelle méthode `suggest_word_new`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "BT2_HfBZXuHg"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_new(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.001,  t = 0.8):\n",
        "\n",
        "    len_context = len(next(iter(n_gram_counts)))\n",
        "\n",
        "    sentence = [\"<s>\"] * len_context\n",
        "    sentence_finished = False\n",
        "\n",
        "    while not sentence_finished:\n",
        "        previous_n_gram = tuple(sentence[-len_context:])\n",
        "        word = suggest_word_new(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k, t)[0]\n",
        "        sentence.append(word)\n",
        "        if word == \"<e>\" or len(sentence) - len_context >= n_words:\n",
        "            sentence_finished = True\n",
        "\n",
        "    return \" \".join(sentence[len_context:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EV6vav7XuHg"
      },
      "source": [
        "#### 5.3.3. Test sur des n-grammes (2 points)\n",
        "Testez ensuite votre fonction avec des 3-grammes et des 5-grammes et validez que les phrases sont plus cohérentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "An1u-pBSXuHh",
        "outputId": "2d7258fb-f891-44aa-fdfb-398a3a0f5041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée avec le modèle tri-gramme :\n",
            "do not have a lot of money and theyre smart\n",
            "\n",
            "Phrase générée avec le modèle 5-gramme :\n",
            "when i see cubans for trump census tackle 285000 establishment\n"
          ]
        }
      ],
      "source": [
        "print(\"Phrase générée avec le modèle tri-gramme :\")\n",
        "print(generate_sentence_new(10, bigram_counts, trigram_counts, voc, t=0.9))\n",
        "print()\n",
        "print(\"Phrase générée avec le modèle 5-gramme :\")\n",
        "print(generate_sentence_new(10, quadrigram_counts, quintigram_counts, voc, t=0.9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_8PvpFxXuHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5PLHOvXuHh"
      },
      "source": [
        "## LIVRABLES:\n",
        "Vous devez remettre sur Moodle, avant la date d'échéance, un zip contenant les fichiers suivants :\n",
        "\n",
        "1-\tLe code : Vous devez compléter le squelette inf8460_tp2.ipynb sous le nom   equipe_i_inf8460_TP2.ipynb (i = votre numéro d’équipe). Indiquez vos noms et matricules au début du notebook. Ce notebook doit contenir les fonctionnalités requises avec des commentaires appropriés. Le code doit être exécutable sans erreur et accompagné de commentaires appropriés de manière à expliquer les différentes fonctions. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ;\n",
        "\n",
        "2-\tUn fichier pdf représentant votre notebook complètement exécuté sous format pdf.\n",
        "Pour créer le fichier cliquez sur File > Download as > PDF via LaTeX (.pdf). Assurez-vous que le PDF est entièrement lisible.\n",
        "\n",
        "\n",
        "## EVALUATION\n",
        "Votre TP sera évalué selon les critères suivants :\n",
        "\n",
        "1. Exécution correcte du code\n",
        "2. Qualité du code (noms significatifs, structure, gestion d’exception, etc.) avec, entre autres, les recommandations suivantes:\n",
        "    - Il ne devrait pas y avoir de duplication de code. Utilisez des fonctions pour garder votre code modulaire\n",
        "    - Votre code devrait être optimisé: un code trop lent entraînera une perte de points\n",
        "3. Lisibilité du code (Commentaires clairs et informatifs au besoin)\n",
        "4. Performance/sortie attendue des modèles\n",
        "5. Réponses correctes/sensées aux questions de réflexion ou d'analyse\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "NLPC2-3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
